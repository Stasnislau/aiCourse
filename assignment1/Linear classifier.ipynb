{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float32) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stani\\AppData\\Local\\Temp\\ipykernel_9084\\3146107952.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(x*x), 2*x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO Implement softmax and cross-entropy for single sample\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_classifer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Make sure it works for big numbers too!\u001b[39;00m\n\u001b[0;32m      4\u001b[0m probs \u001b[38;5;241m=\u001b[39m linear_classifer\u001b[38;5;241m.\u001b[39msoftmax(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\assignment1\\linear_classifer.py:20\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(predictions)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# TODO implement softmax\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Your final implementation shouldn't have any loops\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 20\u001b[0m pred \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m exps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(pred)\n\u001b[0;32m     22\u001b[0m probs \u001b[38;5;241m=\u001b[39m exps \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exps, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_classifer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m linear_classifer\u001b[38;5;241m.\u001b[39mcross_entropy_loss(probs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\assignment1\\linear_classifer.py:20\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(predictions)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# TODO implement softmax\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Your final implementation shouldn't have any loops\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 20\u001b[0m pred \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m exps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(pred)\n\u001b[0;32m     22\u001b[0m probs \u001b[38;5;241m=\u001b[39m exps \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exps, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO Implement combined function or softmax and cross entropy and produces gradient\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss, grad \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_classifer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax_with_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m check_gradient(\u001b[38;5;28;01mlambda\u001b[39;00m x: linear_classifer\u001b[38;5;241m.\u001b[39msoftmax_with_cross_entropy(x, \u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mfloat_))\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\assignment1\\linear_classifer.py:67\u001b[0m, in \u001b[0;36msoftmax_with_cross_entropy\u001b[1;34m(predictions, target_index)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03mComputes softmax and cross-entropy loss for model predictions,\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mincluding the gradient\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m  dprediction, np array same shape as predictions - gradient of predictions by loss value\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# TODO implement softmax with cross-entropy\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Your final implementation shouldn't have any loops\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m dprediction \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(dprediction, target_index)\n\u001b[0;32m     69\u001b[0m dprediction[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(dprediction)), target_index\u001b[38;5;241m.\u001b[39mflatten()] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\assignment1\\linear_classifer.py:20\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(predictions)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# TODO implement softmax\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Your final implementation shouldn't have any loops\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 20\u001b[0m pred \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m exps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(pred)\n\u001b[0;32m     22\u001b[0m probs \u001b[38;5;241m=\u001b[39m exps \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exps, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Everything\\Programming\\Python\\aiCourse\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float_)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int_)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float_)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int_)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float_)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float_)\n",
    "target_index = np.ones(batch_size, dtype=np.int_)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 843.984018\n",
      "Epoch 1, loss: 912.846639\n",
      "Epoch 2, loss: 870.351231\n",
      "Epoch 3, loss: 778.193505\n",
      "Epoch 4, loss: 988.465844\n",
      "Epoch 5, loss: 818.089093\n",
      "Epoch 6, loss: 940.818940\n",
      "Epoch 7, loss: 945.502616\n",
      "Epoch 8, loss: 1119.998584\n",
      "Epoch 9, loss: 967.777656\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x173461350d0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.049\n",
      "Epoch 0, loss: 1017.959183\n",
      "Epoch 1, loss: 998.781755\n",
      "Epoch 2, loss: 933.589755\n",
      "Epoch 3, loss: 976.236960\n",
      "Epoch 4, loss: 908.642106\n",
      "Epoch 5, loss: 1066.702644\n",
      "Epoch 6, loss: 1126.704820\n",
      "Epoch 7, loss: 926.511466\n",
      "Epoch 8, loss: 1070.508103\n",
      "Epoch 9, loss: 949.865213\n",
      "Epoch 10, loss: 1009.697794\n",
      "Epoch 11, loss: 983.563873\n",
      "Epoch 12, loss: 1108.463959\n",
      "Epoch 13, loss: 1090.993175\n",
      "Epoch 14, loss: 1027.385148\n",
      "Epoch 15, loss: 1034.294582\n",
      "Epoch 16, loss: 1061.454530\n",
      "Epoch 17, loss: 1145.104426\n",
      "Epoch 18, loss: 1156.886904\n",
      "Epoch 19, loss: 1196.705165\n",
      "Epoch 20, loss: 1173.103847\n",
      "Epoch 21, loss: 1179.243687\n",
      "Epoch 22, loss: 1364.777271\n",
      "Epoch 23, loss: 1238.204875\n",
      "Epoch 24, loss: 1294.286192\n",
      "Epoch 25, loss: 1280.621419\n",
      "Epoch 26, loss: 1220.849854\n",
      "Epoch 27, loss: 1374.537972\n",
      "Epoch 28, loss: 1300.136547\n",
      "Epoch 29, loss: 1381.755481\n",
      "Epoch 30, loss: 1398.523673\n",
      "Epoch 31, loss: 1335.507229\n",
      "Epoch 32, loss: 1189.197885\n",
      "Epoch 33, loss: 1497.833115\n",
      "Epoch 34, loss: 1406.037887\n",
      "Epoch 35, loss: 1291.300054\n",
      "Epoch 36, loss: 1339.445245\n",
      "Epoch 37, loss: 1482.167336\n",
      "Epoch 38, loss: 1518.327001\n",
      "Epoch 39, loss: 1522.443660\n",
      "Epoch 40, loss: 1437.142017\n",
      "Epoch 41, loss: 1438.226490\n",
      "Epoch 42, loss: 1453.506592\n",
      "Epoch 43, loss: 1441.577032\n",
      "Epoch 44, loss: 1529.229806\n",
      "Epoch 45, loss: 1754.850770\n",
      "Epoch 46, loss: 1587.502065\n",
      "Epoch 47, loss: 1519.572911\n",
      "Epoch 48, loss: 1625.510660\n",
      "Epoch 49, loss: 1575.717144\n",
      "Epoch 50, loss: 1613.630332\n",
      "Epoch 51, loss: 1466.859875\n",
      "Epoch 52, loss: 1729.179534\n",
      "Epoch 53, loss: 1574.061449\n",
      "Epoch 54, loss: 1654.931847\n",
      "Epoch 55, loss: 1605.626592\n",
      "Epoch 56, loss: 1794.978129\n",
      "Epoch 57, loss: 1805.141008\n",
      "Epoch 58, loss: 1688.729806\n",
      "Epoch 59, loss: 1945.106640\n",
      "Epoch 60, loss: 1843.551351\n",
      "Epoch 61, loss: 1658.607405\n",
      "Epoch 62, loss: 1938.470674\n",
      "Epoch 63, loss: 2000.868961\n",
      "Epoch 64, loss: 1753.764988\n",
      "Epoch 65, loss: 1894.375069\n",
      "Epoch 66, loss: 1811.943681\n",
      "Epoch 67, loss: 1881.636525\n",
      "Epoch 68, loss: 1868.588197\n",
      "Epoch 69, loss: 1909.023425\n",
      "Epoch 70, loss: 1823.829349\n",
      "Epoch 71, loss: 1890.825498\n",
      "Epoch 72, loss: 2051.253892\n",
      "Epoch 73, loss: 1894.644153\n",
      "Epoch 74, loss: 2003.479380\n",
      "Epoch 75, loss: 2054.597875\n",
      "Epoch 76, loss: 1905.193929\n",
      "Epoch 77, loss: 2018.139592\n",
      "Epoch 78, loss: 1949.792578\n",
      "Epoch 79, loss: 2082.759416\n",
      "Epoch 80, loss: 2015.302360\n",
      "Epoch 81, loss: 2104.836366\n",
      "Epoch 82, loss: 2029.107330\n",
      "Epoch 83, loss: 2053.128866\n",
      "Epoch 84, loss: 2252.058714\n",
      "Epoch 85, loss: 2264.843676\n",
      "Epoch 86, loss: 2200.223581\n",
      "Epoch 87, loss: 2293.096973\n",
      "Epoch 88, loss: 2255.112047\n",
      "Epoch 89, loss: 2160.470425\n",
      "Epoch 90, loss: 2347.630112\n",
      "Epoch 91, loss: 2314.249140\n",
      "Epoch 92, loss: 2341.585206\n",
      "Epoch 93, loss: 2443.372956\n",
      "Epoch 94, loss: 2290.040928\n",
      "Epoch 95, loss: 2225.663024\n",
      "Epoch 96, loss: 2499.894369\n",
      "Epoch 97, loss: 2488.913976\n",
      "Epoch 98, loss: 2385.097770\n",
      "Epoch 99, loss: 2306.854852\n",
      "Accuracy after training for 100 epochs:  0.049\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 690.702340\n",
      "Epoch 1, loss: 690.499616\n",
      "Epoch 2, loss: 690.221683\n",
      "Epoch 3, loss: 690.173639\n",
      "Epoch 4, loss: 690.132389\n",
      "Epoch 5, loss: 690.521815\n",
      "Epoch 6, loss: 690.792944\n",
      "Epoch 7, loss: 689.655404\n",
      "Epoch 8, loss: 690.495612\n",
      "Epoch 9, loss: 689.663429\n",
      "Epoch 10, loss: 689.780806\n",
      "Epoch 11, loss: 688.943291\n",
      "Epoch 12, loss: 689.017843\n",
      "Epoch 13, loss: 689.784818\n",
      "Epoch 14, loss: 689.460553\n",
      "Epoch 15, loss: 689.514948\n",
      "Epoch 16, loss: 689.779114\n",
      "Epoch 17, loss: 689.251579\n",
      "Epoch 18, loss: 689.202355\n",
      "Epoch 19, loss: 689.356431\n",
      "Epoch 20, loss: 688.761534\n",
      "Epoch 21, loss: 688.780403\n",
      "Epoch 22, loss: 688.170116\n",
      "Epoch 23, loss: 688.282245\n",
      "Epoch 24, loss: 688.403687\n",
      "Epoch 25, loss: 689.224060\n",
      "Epoch 26, loss: 688.902295\n",
      "Epoch 27, loss: 687.045213\n",
      "Epoch 28, loss: 687.750020\n",
      "Epoch 29, loss: 688.338849\n",
      "Epoch 30, loss: 689.136060\n",
      "Epoch 31, loss: 688.325437\n",
      "Epoch 32, loss: 688.490900\n",
      "Epoch 33, loss: 689.177791\n",
      "Epoch 34, loss: 688.246300\n",
      "Epoch 35, loss: 688.174119\n",
      "Epoch 36, loss: 686.877287\n",
      "Epoch 37, loss: 688.932221\n",
      "Epoch 38, loss: 688.810512\n",
      "Epoch 39, loss: 687.172710\n",
      "Epoch 40, loss: 688.032976\n",
      "Epoch 41, loss: 688.159605\n",
      "Epoch 42, loss: 687.701624\n",
      "Epoch 43, loss: 686.432179\n",
      "Epoch 44, loss: 689.286503\n",
      "Epoch 45, loss: 686.556622\n",
      "Epoch 46, loss: 688.041029\n",
      "Epoch 47, loss: 688.534550\n",
      "Epoch 48, loss: 686.018512\n",
      "Epoch 49, loss: 684.558454\n",
      "Epoch 50, loss: 686.945327\n",
      "Epoch 51, loss: 686.894301\n",
      "Epoch 52, loss: 685.765769\n",
      "Epoch 53, loss: 686.403948\n",
      "Epoch 54, loss: 686.880649\n",
      "Epoch 55, loss: 686.917858\n",
      "Epoch 56, loss: 685.747958\n",
      "Epoch 57, loss: 685.786573\n",
      "Epoch 58, loss: 686.144327\n",
      "Epoch 59, loss: 685.638167\n",
      "Epoch 60, loss: 686.210408\n",
      "Epoch 61, loss: 685.514615\n",
      "Epoch 62, loss: 685.204191\n",
      "Epoch 63, loss: 685.835839\n",
      "Epoch 64, loss: 685.625195\n",
      "Epoch 65, loss: 686.875207\n",
      "Epoch 66, loss: 686.286281\n",
      "Epoch 67, loss: 685.831102\n",
      "Epoch 68, loss: 686.740756\n",
      "Epoch 69, loss: 685.296872\n",
      "Epoch 70, loss: 685.290011\n",
      "Epoch 71, loss: 684.755367\n",
      "Epoch 72, loss: 684.911370\n",
      "Epoch 73, loss: 684.019113\n",
      "Epoch 74, loss: 684.677965\n",
      "Epoch 75, loss: 685.568644\n",
      "Epoch 76, loss: 681.484684\n",
      "Epoch 77, loss: 685.895838\n",
      "Epoch 78, loss: 686.090998\n",
      "Epoch 79, loss: 685.004199\n",
      "Epoch 80, loss: 683.713469\n",
      "Epoch 81, loss: 683.768730\n",
      "Epoch 82, loss: 684.547325\n",
      "Epoch 83, loss: 686.791900\n",
      "Epoch 84, loss: 683.929597\n",
      "Epoch 85, loss: 686.895406\n",
      "Epoch 86, loss: 683.863555\n",
      "Epoch 87, loss: 684.352641\n",
      "Epoch 88, loss: 684.893914\n",
      "Epoch 89, loss: 682.801028\n",
      "Epoch 90, loss: 686.062009\n",
      "Epoch 91, loss: 684.884436\n",
      "Epoch 92, loss: 684.127005\n",
      "Epoch 93, loss: 687.048787\n",
      "Epoch 94, loss: 682.343918\n",
      "Epoch 95, loss: 682.315893\n",
      "Epoch 96, loss: 685.376504\n",
      "Epoch 97, loss: 685.791250\n",
      "Epoch 98, loss: 684.525361\n",
      "Epoch 99, loss: 683.363694\n",
      "Epoch 100, loss: 684.698748\n",
      "Epoch 101, loss: 683.709538\n",
      "Epoch 102, loss: 682.751335\n",
      "Epoch 103, loss: 681.280089\n",
      "Epoch 104, loss: 682.059030\n",
      "Epoch 105, loss: 679.781449\n",
      "Epoch 106, loss: 684.457332\n",
      "Epoch 107, loss: 682.798201\n",
      "Epoch 108, loss: 683.609386\n",
      "Epoch 109, loss: 682.882147\n",
      "Epoch 110, loss: 681.245312\n",
      "Epoch 111, loss: 684.617646\n",
      "Epoch 112, loss: 682.357325\n",
      "Epoch 113, loss: 682.480866\n",
      "Epoch 114, loss: 681.707533\n",
      "Epoch 115, loss: 681.476732\n",
      "Epoch 116, loss: 685.076924\n",
      "Epoch 117, loss: 684.147573\n",
      "Epoch 118, loss: 681.586079\n",
      "Epoch 119, loss: 681.357476\n",
      "Epoch 120, loss: 684.216880\n",
      "Epoch 121, loss: 683.470049\n",
      "Epoch 122, loss: 681.097526\n",
      "Epoch 123, loss: 685.469843\n",
      "Epoch 124, loss: 685.056340\n",
      "Epoch 125, loss: 681.150872\n",
      "Epoch 126, loss: 684.395805\n",
      "Epoch 127, loss: 682.571107\n",
      "Epoch 128, loss: 680.229994\n",
      "Epoch 129, loss: 683.101622\n",
      "Epoch 130, loss: 681.912966\n",
      "Epoch 131, loss: 682.198416\n",
      "Epoch 132, loss: 680.099484\n",
      "Epoch 133, loss: 680.058717\n",
      "Epoch 134, loss: 682.613922\n",
      "Epoch 135, loss: 681.606909\n",
      "Epoch 136, loss: 680.600128\n",
      "Epoch 137, loss: 680.875458\n",
      "Epoch 138, loss: 680.182087\n",
      "Epoch 139, loss: 681.430171\n",
      "Epoch 140, loss: 681.723602\n",
      "Epoch 141, loss: 680.942327\n",
      "Epoch 142, loss: 679.159750\n",
      "Epoch 143, loss: 679.304657\n",
      "Epoch 144, loss: 681.340797\n",
      "Epoch 145, loss: 681.127538\n",
      "Epoch 146, loss: 680.311490\n",
      "Epoch 147, loss: 680.396123\n",
      "Epoch 148, loss: 681.466031\n",
      "Epoch 149, loss: 681.299253\n",
      "Epoch 150, loss: 678.381017\n",
      "Epoch 151, loss: 680.859689\n",
      "Epoch 152, loss: 678.686846\n",
      "Epoch 153, loss: 683.096575\n",
      "Epoch 154, loss: 680.285562\n",
      "Epoch 155, loss: 680.396515\n",
      "Epoch 156, loss: 676.207680\n",
      "Epoch 157, loss: 677.837664\n",
      "Epoch 158, loss: 680.161010\n",
      "Epoch 159, loss: 679.101564\n",
      "Epoch 160, loss: 681.367864\n",
      "Epoch 161, loss: 683.054752\n",
      "Epoch 162, loss: 681.798160\n",
      "Epoch 163, loss: 681.670728\n",
      "Epoch 164, loss: 678.307429\n",
      "Epoch 165, loss: 678.292301\n",
      "Epoch 166, loss: 679.972320\n",
      "Epoch 167, loss: 676.862783\n",
      "Epoch 168, loss: 675.572048\n",
      "Epoch 169, loss: 678.363532\n",
      "Epoch 170, loss: 677.294382\n",
      "Epoch 171, loss: 677.333834\n",
      "Epoch 172, loss: 676.298902\n",
      "Epoch 173, loss: 677.909239\n",
      "Epoch 174, loss: 680.443827\n",
      "Epoch 175, loss: 679.980416\n",
      "Epoch 176, loss: 679.887630\n",
      "Epoch 177, loss: 679.374715\n",
      "Epoch 178, loss: 678.394027\n",
      "Epoch 179, loss: 681.731243\n",
      "Epoch 180, loss: 677.557261\n",
      "Epoch 181, loss: 678.363141\n",
      "Epoch 182, loss: 678.163158\n",
      "Epoch 183, loss: 679.471928\n",
      "Epoch 184, loss: 679.211067\n",
      "Epoch 185, loss: 677.707855\n",
      "Epoch 186, loss: 681.257728\n",
      "Epoch 187, loss: 679.388795\n",
      "Epoch 188, loss: 681.228975\n",
      "Epoch 189, loss: 675.101816\n",
      "Epoch 190, loss: 678.963593\n",
      "Epoch 191, loss: 677.769394\n",
      "Epoch 192, loss: 674.941298\n",
      "Epoch 193, loss: 677.935570\n",
      "Epoch 194, loss: 678.988869\n",
      "Epoch 195, loss: 678.029953\n",
      "Epoch 196, loss: 678.849339\n",
      "Epoch 197, loss: 677.951743\n",
      "Epoch 198, loss: 680.043097\n",
      "Epoch 199, loss: 676.914185\n",
      "Validation accuracy:  0.207 lr:  1e-06 reg:  1e-06\n",
      "Epoch 0, loss: 690.941729\n",
      "Epoch 1, loss: 690.805580\n",
      "Epoch 2, loss: 690.546157\n",
      "Epoch 3, loss: 690.130844\n",
      "Epoch 4, loss: 689.989007\n",
      "Epoch 5, loss: 690.421323\n",
      "Epoch 6, loss: 690.181288\n",
      "Epoch 7, loss: 690.181391\n",
      "Epoch 8, loss: 689.781363\n",
      "Epoch 9, loss: 689.941941\n",
      "Epoch 10, loss: 689.381760\n",
      "Epoch 11, loss: 689.208479\n",
      "Epoch 12, loss: 688.669639\n",
      "Epoch 13, loss: 689.781062\n",
      "Epoch 14, loss: 689.320309\n",
      "Epoch 15, loss: 690.520700\n",
      "Epoch 16, loss: 689.377552\n",
      "Epoch 17, loss: 688.387397\n",
      "Epoch 18, loss: 688.328785\n",
      "Epoch 19, loss: 688.716356\n",
      "Epoch 20, loss: 688.197656\n",
      "Epoch 21, loss: 689.027263\n",
      "Epoch 22, loss: 688.721872\n",
      "Epoch 23, loss: 688.837491\n",
      "Epoch 24, loss: 689.273311\n",
      "Epoch 25, loss: 689.343445\n",
      "Epoch 26, loss: 687.983974\n",
      "Epoch 27, loss: 689.086137\n",
      "Epoch 28, loss: 689.063750\n",
      "Epoch 29, loss: 687.281967\n",
      "Epoch 30, loss: 687.559740\n",
      "Epoch 31, loss: 687.650286\n",
      "Epoch 32, loss: 687.900186\n",
      "Epoch 33, loss: 688.420061\n",
      "Epoch 34, loss: 687.205228\n",
      "Epoch 35, loss: 688.599600\n",
      "Epoch 36, loss: 687.438927\n",
      "Epoch 37, loss: 688.414042\n",
      "Epoch 38, loss: 688.482674\n",
      "Epoch 39, loss: 687.685050\n",
      "Epoch 40, loss: 688.110113\n",
      "Epoch 41, loss: 687.888014\n",
      "Epoch 42, loss: 687.899676\n",
      "Epoch 43, loss: 685.734841\n",
      "Epoch 44, loss: 687.299201\n",
      "Epoch 45, loss: 687.482844\n",
      "Epoch 46, loss: 686.928617\n",
      "Epoch 47, loss: 686.034930\n",
      "Epoch 48, loss: 685.791783\n",
      "Epoch 49, loss: 687.430727\n",
      "Epoch 50, loss: 687.238949\n",
      "Epoch 51, loss: 687.844306\n",
      "Epoch 52, loss: 684.791945\n",
      "Epoch 53, loss: 686.085745\n",
      "Epoch 54, loss: 686.911841\n",
      "Epoch 55, loss: 687.047476\n",
      "Epoch 56, loss: 685.694767\n",
      "Epoch 57, loss: 686.488118\n",
      "Epoch 58, loss: 685.933072\n",
      "Epoch 59, loss: 686.631329\n",
      "Epoch 60, loss: 685.754443\n",
      "Epoch 61, loss: 687.605212\n",
      "Epoch 62, loss: 686.245944\n",
      "Epoch 63, loss: 686.302338\n",
      "Epoch 64, loss: 685.261053\n",
      "Epoch 65, loss: 685.535852\n",
      "Epoch 66, loss: 683.062609\n",
      "Epoch 67, loss: 684.489910\n",
      "Epoch 68, loss: 685.806413\n",
      "Epoch 69, loss: 683.854475\n",
      "Epoch 70, loss: 686.698920\n",
      "Epoch 71, loss: 683.373014\n",
      "Epoch 72, loss: 684.722481\n",
      "Epoch 73, loss: 685.742229\n",
      "Epoch 74, loss: 684.826263\n",
      "Epoch 75, loss: 685.833813\n",
      "Epoch 76, loss: 685.101789\n",
      "Epoch 77, loss: 684.591169\n",
      "Epoch 78, loss: 684.846157\n",
      "Epoch 79, loss: 684.442965\n",
      "Epoch 80, loss: 684.749917\n",
      "Epoch 81, loss: 684.311094\n",
      "Epoch 82, loss: 685.694600\n",
      "Epoch 83, loss: 685.265161\n",
      "Epoch 84, loss: 683.538334\n",
      "Epoch 85, loss: 684.070671\n",
      "Epoch 86, loss: 682.266748\n",
      "Epoch 87, loss: 683.379451\n",
      "Epoch 88, loss: 684.277718\n",
      "Epoch 89, loss: 683.819706\n",
      "Epoch 90, loss: 682.439313\n",
      "Epoch 91, loss: 682.995263\n",
      "Epoch 92, loss: 684.004457\n",
      "Epoch 93, loss: 684.303717\n",
      "Epoch 94, loss: 682.478546\n",
      "Epoch 95, loss: 685.164998\n",
      "Epoch 96, loss: 685.471857\n",
      "Epoch 97, loss: 682.378921\n",
      "Epoch 98, loss: 682.774956\n",
      "Epoch 99, loss: 684.563290\n",
      "Epoch 100, loss: 683.941057\n",
      "Epoch 101, loss: 682.633734\n",
      "Epoch 102, loss: 681.336093\n",
      "Epoch 103, loss: 682.404367\n",
      "Epoch 104, loss: 682.306112\n",
      "Epoch 105, loss: 682.068358\n",
      "Epoch 106, loss: 684.048849\n",
      "Epoch 107, loss: 680.933237\n",
      "Epoch 108, loss: 679.951435\n",
      "Epoch 109, loss: 683.027341\n",
      "Epoch 110, loss: 681.859157\n",
      "Epoch 111, loss: 682.721660\n",
      "Epoch 112, loss: 683.752823\n",
      "Epoch 113, loss: 683.155431\n",
      "Epoch 114, loss: 681.835478\n",
      "Epoch 115, loss: 682.909880\n",
      "Epoch 116, loss: 683.753807\n",
      "Epoch 117, loss: 681.037738\n",
      "Epoch 118, loss: 681.752973\n",
      "Epoch 119, loss: 684.457829\n",
      "Epoch 120, loss: 680.041378\n",
      "Epoch 121, loss: 680.211171\n",
      "Epoch 122, loss: 681.243014\n",
      "Epoch 123, loss: 683.960484\n",
      "Epoch 124, loss: 683.345649\n",
      "Epoch 125, loss: 683.010584\n",
      "Epoch 126, loss: 681.562702\n",
      "Epoch 127, loss: 679.453179\n",
      "Epoch 128, loss: 681.792549\n",
      "Epoch 129, loss: 682.789459\n",
      "Epoch 130, loss: 682.509418\n",
      "Epoch 131, loss: 681.852440\n",
      "Epoch 132, loss: 682.027744\n",
      "Epoch 133, loss: 680.973076\n",
      "Epoch 134, loss: 680.993626\n",
      "Epoch 135, loss: 680.669805\n",
      "Epoch 136, loss: 678.775817\n",
      "Epoch 137, loss: 681.282917\n",
      "Epoch 138, loss: 681.125326\n",
      "Epoch 139, loss: 682.665440\n",
      "Epoch 140, loss: 682.202491\n",
      "Epoch 141, loss: 680.409741\n",
      "Epoch 142, loss: 678.479581\n",
      "Epoch 143, loss: 680.386641\n",
      "Epoch 144, loss: 678.499353\n",
      "Epoch 145, loss: 677.327412\n",
      "Epoch 146, loss: 681.932938\n",
      "Epoch 147, loss: 680.806350\n",
      "Epoch 148, loss: 679.756340\n",
      "Epoch 149, loss: 678.146515\n",
      "Epoch 150, loss: 680.758925\n",
      "Epoch 151, loss: 681.996154\n",
      "Epoch 152, loss: 682.177879\n",
      "Epoch 153, loss: 680.225383\n",
      "Epoch 154, loss: 680.933537\n",
      "Epoch 155, loss: 679.551533\n",
      "Epoch 156, loss: 679.437954\n",
      "Epoch 157, loss: 681.781083\n",
      "Epoch 158, loss: 678.459078\n",
      "Epoch 159, loss: 676.286594\n",
      "Epoch 160, loss: 681.119401\n",
      "Epoch 161, loss: 678.662502\n",
      "Epoch 162, loss: 676.722240\n",
      "Epoch 163, loss: 681.185714\n",
      "Epoch 164, loss: 679.152564\n",
      "Epoch 165, loss: 678.567620\n",
      "Epoch 166, loss: 681.238113\n",
      "Epoch 167, loss: 680.740899\n",
      "Epoch 168, loss: 681.511033\n",
      "Epoch 169, loss: 678.992359\n",
      "Epoch 170, loss: 679.547501\n",
      "Epoch 171, loss: 683.021352\n",
      "Epoch 172, loss: 680.541962\n",
      "Epoch 173, loss: 680.538562\n",
      "Epoch 174, loss: 679.953794\n",
      "Epoch 175, loss: 680.260425\n",
      "Epoch 176, loss: 680.094603\n",
      "Epoch 177, loss: 677.228456\n",
      "Epoch 178, loss: 675.704851\n",
      "Epoch 179, loss: 680.295473\n",
      "Epoch 180, loss: 682.263975\n",
      "Epoch 181, loss: 680.689280\n",
      "Epoch 182, loss: 677.003312\n",
      "Epoch 183, loss: 676.796579\n",
      "Epoch 184, loss: 678.071645\n",
      "Epoch 185, loss: 674.571957\n",
      "Epoch 186, loss: 676.885545\n",
      "Epoch 187, loss: 679.983339\n",
      "Epoch 188, loss: 676.166199\n",
      "Epoch 189, loss: 672.320190\n",
      "Epoch 190, loss: 679.779482\n",
      "Epoch 191, loss: 678.739359\n",
      "Epoch 192, loss: 676.873726\n",
      "Epoch 193, loss: 675.986436\n",
      "Epoch 194, loss: 676.705881\n",
      "Epoch 195, loss: 675.663565\n",
      "Epoch 196, loss: 676.211542\n",
      "Epoch 197, loss: 679.483229\n",
      "Epoch 198, loss: 678.300918\n",
      "Epoch 199, loss: 678.189192\n",
      "Validation accuracy:  0.209 lr:  1e-06 reg:  1e-05\n",
      "Epoch 0, loss: 690.736367\n",
      "Epoch 1, loss: 690.251604\n",
      "Epoch 2, loss: 690.935151\n",
      "Epoch 3, loss: 690.197474\n",
      "Epoch 4, loss: 690.088656\n",
      "Epoch 5, loss: 690.331559\n",
      "Epoch 6, loss: 690.541999\n",
      "Epoch 7, loss: 690.260482\n",
      "Epoch 8, loss: 690.648420\n",
      "Epoch 9, loss: 689.639086\n",
      "Epoch 10, loss: 689.729275\n",
      "Epoch 11, loss: 689.652254\n",
      "Epoch 12, loss: 689.953345\n",
      "Epoch 13, loss: 689.891632\n",
      "Epoch 14, loss: 689.416776\n",
      "Epoch 15, loss: 689.570778\n",
      "Epoch 16, loss: 689.870844\n",
      "Epoch 17, loss: 688.952879\n",
      "Epoch 18, loss: 688.867554\n",
      "Epoch 19, loss: 689.030410\n",
      "Epoch 20, loss: 689.856635\n",
      "Epoch 21, loss: 688.703145\n",
      "Epoch 22, loss: 688.754011\n",
      "Epoch 23, loss: 688.191015\n",
      "Epoch 24, loss: 688.515971\n",
      "Epoch 25, loss: 688.221541\n",
      "Epoch 26, loss: 688.942116\n",
      "Epoch 27, loss: 687.491770\n",
      "Epoch 28, loss: 687.433773\n",
      "Epoch 29, loss: 687.987056\n",
      "Epoch 30, loss: 689.371142\n",
      "Epoch 31, loss: 688.525996\n",
      "Epoch 32, loss: 688.432213\n",
      "Epoch 33, loss: 688.705173\n",
      "Epoch 34, loss: 687.405112\n",
      "Epoch 35, loss: 688.521817\n",
      "Epoch 36, loss: 688.147921\n",
      "Epoch 37, loss: 687.553542\n",
      "Epoch 38, loss: 687.728321\n",
      "Epoch 39, loss: 688.554200\n",
      "Epoch 40, loss: 686.539374\n",
      "Epoch 41, loss: 688.733005\n",
      "Epoch 42, loss: 688.184827\n",
      "Epoch 43, loss: 688.391410\n",
      "Epoch 44, loss: 686.572455\n",
      "Epoch 45, loss: 687.665213\n",
      "Epoch 46, loss: 686.606745\n",
      "Epoch 47, loss: 686.861493\n",
      "Epoch 48, loss: 685.439212\n",
      "Epoch 49, loss: 686.300526\n",
      "Epoch 50, loss: 688.145499\n",
      "Epoch 51, loss: 686.464099\n",
      "Epoch 52, loss: 686.363765\n",
      "Epoch 53, loss: 686.455092\n",
      "Epoch 54, loss: 686.675560\n",
      "Epoch 55, loss: 686.866246\n",
      "Epoch 56, loss: 685.630666\n",
      "Epoch 57, loss: 686.977106\n",
      "Epoch 58, loss: 685.978341\n",
      "Epoch 59, loss: 686.104558\n",
      "Epoch 60, loss: 684.444025\n",
      "Epoch 61, loss: 685.895022\n",
      "Epoch 62, loss: 684.051690\n",
      "Epoch 63, loss: 686.913746\n",
      "Epoch 64, loss: 686.255357\n",
      "Epoch 65, loss: 685.440156\n",
      "Epoch 66, loss: 685.264805\n",
      "Epoch 67, loss: 685.864261\n",
      "Epoch 68, loss: 685.998659\n",
      "Epoch 69, loss: 686.290796\n",
      "Epoch 70, loss: 685.515322\n",
      "Epoch 71, loss: 685.215844\n",
      "Epoch 72, loss: 683.421323\n",
      "Epoch 73, loss: 686.765354\n",
      "Epoch 74, loss: 685.465342\n",
      "Epoch 75, loss: 685.701037\n",
      "Epoch 76, loss: 685.108130\n",
      "Epoch 77, loss: 684.037401\n",
      "Epoch 78, loss: 685.973640\n",
      "Epoch 79, loss: 684.425768\n",
      "Epoch 80, loss: 685.347967\n",
      "Epoch 81, loss: 683.894451\n",
      "Epoch 82, loss: 684.077069\n",
      "Epoch 83, loss: 685.204591\n",
      "Epoch 84, loss: 683.818572\n",
      "Epoch 85, loss: 684.481460\n",
      "Epoch 86, loss: 683.720541\n",
      "Epoch 87, loss: 684.730704\n",
      "Epoch 88, loss: 683.877071\n",
      "Epoch 89, loss: 684.613319\n",
      "Epoch 90, loss: 684.769975\n",
      "Epoch 91, loss: 682.121366\n",
      "Epoch 92, loss: 683.626175\n",
      "Epoch 93, loss: 682.132773\n",
      "Epoch 94, loss: 683.387266\n",
      "Epoch 95, loss: 684.277724\n",
      "Epoch 96, loss: 682.341585\n",
      "Epoch 97, loss: 685.443165\n",
      "Epoch 98, loss: 681.916294\n",
      "Epoch 99, loss: 684.429787\n",
      "Epoch 100, loss: 682.785958\n",
      "Epoch 101, loss: 681.338385\n",
      "Epoch 102, loss: 681.963985\n",
      "Epoch 103, loss: 681.998764\n",
      "Epoch 104, loss: 684.202061\n",
      "Epoch 105, loss: 680.638988\n",
      "Epoch 106, loss: 683.673451\n",
      "Epoch 107, loss: 685.152042\n",
      "Epoch 108, loss: 681.290122\n",
      "Epoch 109, loss: 681.347057\n",
      "Epoch 110, loss: 681.851132\n",
      "Epoch 111, loss: 682.390887\n",
      "Epoch 112, loss: 682.238132\n",
      "Epoch 113, loss: 682.530585\n",
      "Epoch 114, loss: 683.250524\n",
      "Epoch 115, loss: 682.722879\n",
      "Epoch 116, loss: 680.997720\n",
      "Epoch 117, loss: 681.442517\n",
      "Epoch 118, loss: 684.324261\n",
      "Epoch 119, loss: 681.009917\n",
      "Epoch 120, loss: 680.413879\n",
      "Epoch 121, loss: 682.140775\n",
      "Epoch 122, loss: 682.463184\n",
      "Epoch 123, loss: 681.207955\n",
      "Epoch 124, loss: 678.720157\n",
      "Epoch 125, loss: 682.920116\n",
      "Epoch 126, loss: 683.470301\n",
      "Epoch 127, loss: 682.285127\n",
      "Epoch 128, loss: 680.833847\n",
      "Epoch 129, loss: 681.675478\n",
      "Epoch 130, loss: 678.815231\n",
      "Epoch 131, loss: 680.952133\n",
      "Epoch 132, loss: 679.858971\n",
      "Epoch 133, loss: 678.452269\n",
      "Epoch 134, loss: 679.676661\n",
      "Epoch 135, loss: 681.879645\n",
      "Epoch 136, loss: 681.536644\n",
      "Epoch 137, loss: 681.442211\n",
      "Epoch 138, loss: 677.002288\n",
      "Epoch 139, loss: 678.155034\n",
      "Epoch 140, loss: 680.948847\n",
      "Epoch 141, loss: 681.559479\n",
      "Epoch 142, loss: 681.066210\n",
      "Epoch 143, loss: 679.104468\n",
      "Epoch 144, loss: 681.507808\n",
      "Epoch 145, loss: 682.022167\n",
      "Epoch 146, loss: 679.414146\n",
      "Epoch 147, loss: 681.659624\n",
      "Epoch 148, loss: 678.418687\n",
      "Epoch 149, loss: 679.736123\n",
      "Epoch 150, loss: 681.864041\n",
      "Epoch 151, loss: 680.979839\n",
      "Epoch 152, loss: 680.640911\n",
      "Epoch 153, loss: 679.060542\n",
      "Epoch 154, loss: 677.327938\n",
      "Epoch 155, loss: 677.587914\n",
      "Epoch 156, loss: 680.631279\n",
      "Epoch 157, loss: 682.680902\n",
      "Epoch 158, loss: 682.499791\n",
      "Epoch 159, loss: 678.137362\n",
      "Epoch 160, loss: 679.214562\n",
      "Epoch 161, loss: 679.597452\n",
      "Epoch 162, loss: 680.079228\n",
      "Epoch 163, loss: 679.411119\n",
      "Epoch 164, loss: 681.497382\n",
      "Epoch 165, loss: 679.614359\n",
      "Epoch 166, loss: 677.295163\n",
      "Epoch 167, loss: 682.350728\n",
      "Epoch 168, loss: 678.034796\n",
      "Epoch 169, loss: 677.873253\n",
      "Epoch 170, loss: 681.421916\n",
      "Epoch 171, loss: 678.972426\n",
      "Epoch 172, loss: 681.483878\n",
      "Epoch 173, loss: 677.421404\n",
      "Epoch 174, loss: 680.328853\n",
      "Epoch 175, loss: 679.944788\n",
      "Epoch 176, loss: 677.939541\n",
      "Epoch 177, loss: 677.990505\n",
      "Epoch 178, loss: 677.224762\n",
      "Epoch 179, loss: 677.229195\n",
      "Epoch 180, loss: 679.804703\n",
      "Epoch 181, loss: 678.236083\n",
      "Epoch 182, loss: 676.691836\n",
      "Epoch 183, loss: 680.542591\n",
      "Epoch 184, loss: 677.583550\n",
      "Epoch 185, loss: 683.396886\n",
      "Epoch 186, loss: 678.929070\n",
      "Epoch 187, loss: 680.455713\n",
      "Epoch 188, loss: 677.725221\n",
      "Epoch 189, loss: 677.186259\n",
      "Epoch 190, loss: 677.111951\n",
      "Epoch 191, loss: 677.271014\n",
      "Epoch 192, loss: 676.529465\n",
      "Epoch 193, loss: 677.615456\n",
      "Epoch 194, loss: 682.155558\n",
      "Epoch 195, loss: 680.120706\n",
      "Epoch 196, loss: 678.483871\n",
      "Epoch 197, loss: 676.170639\n",
      "Epoch 198, loss: 677.711294\n",
      "Epoch 199, loss: 681.685706\n",
      "Validation accuracy:  0.21 lr:  1e-06 reg:  0.0001\n",
      "Epoch 0, loss: 690.737588\n",
      "Epoch 1, loss: 690.419798\n",
      "Epoch 2, loss: 690.209425\n",
      "Epoch 3, loss: 690.326903\n",
      "Epoch 4, loss: 690.444587\n",
      "Epoch 5, loss: 690.216048\n",
      "Epoch 6, loss: 690.271404\n",
      "Epoch 7, loss: 690.622157\n",
      "Epoch 8, loss: 689.762179\n",
      "Epoch 9, loss: 689.841176\n",
      "Epoch 10, loss: 690.140287\n",
      "Epoch 11, loss: 689.372278\n",
      "Epoch 12, loss: 690.062890\n",
      "Epoch 13, loss: 689.054675\n",
      "Epoch 14, loss: 689.559519\n",
      "Epoch 15, loss: 689.895597\n",
      "Epoch 16, loss: 689.875940\n",
      "Epoch 17, loss: 689.540375\n",
      "Epoch 18, loss: 689.591947\n",
      "Epoch 19, loss: 688.806114\n",
      "Epoch 20, loss: 689.063385\n",
      "Epoch 21, loss: 688.346021\n",
      "Epoch 22, loss: 688.809769\n",
      "Epoch 23, loss: 689.053062\n",
      "Epoch 24, loss: 688.181427\n",
      "Epoch 25, loss: 688.665379\n",
      "Epoch 26, loss: 689.000127\n",
      "Epoch 27, loss: 688.303127\n",
      "Epoch 28, loss: 687.430047\n",
      "Epoch 29, loss: 688.435636\n",
      "Epoch 30, loss: 688.703868\n",
      "Epoch 31, loss: 688.124162\n",
      "Epoch 32, loss: 687.084420\n",
      "Epoch 33, loss: 687.185888\n",
      "Epoch 34, loss: 688.070125\n",
      "Epoch 35, loss: 687.981888\n",
      "Epoch 36, loss: 688.429119\n",
      "Epoch 37, loss: 687.656146\n",
      "Epoch 38, loss: 686.909513\n",
      "Epoch 39, loss: 686.167615\n",
      "Epoch 40, loss: 688.423019\n",
      "Epoch 41, loss: 687.071704\n",
      "Epoch 42, loss: 687.779687\n",
      "Epoch 43, loss: 687.554945\n",
      "Epoch 44, loss: 687.489230\n",
      "Epoch 45, loss: 688.143364\n",
      "Epoch 46, loss: 686.965769\n",
      "Epoch 47, loss: 687.019847\n",
      "Epoch 48, loss: 685.836362\n",
      "Epoch 49, loss: 686.942674\n",
      "Epoch 50, loss: 686.684598\n",
      "Epoch 51, loss: 686.074234\n",
      "Epoch 52, loss: 686.567325\n",
      "Epoch 53, loss: 685.991878\n",
      "Epoch 54, loss: 686.391426\n",
      "Epoch 55, loss: 686.829862\n",
      "Epoch 56, loss: 686.538943\n",
      "Epoch 57, loss: 687.769578\n",
      "Epoch 58, loss: 686.928014\n",
      "Epoch 59, loss: 687.729286\n",
      "Epoch 60, loss: 685.814406\n",
      "Epoch 61, loss: 685.785347\n",
      "Epoch 62, loss: 686.428127\n",
      "Epoch 63, loss: 684.498498\n",
      "Epoch 64, loss: 685.886761\n",
      "Epoch 65, loss: 685.806706\n",
      "Epoch 66, loss: 684.813518\n",
      "Epoch 67, loss: 684.530693\n",
      "Epoch 68, loss: 686.747180\n",
      "Epoch 69, loss: 686.219880\n",
      "Epoch 70, loss: 685.602800\n",
      "Epoch 71, loss: 684.389048\n",
      "Epoch 72, loss: 685.689053\n",
      "Epoch 73, loss: 684.440604\n",
      "Epoch 74, loss: 685.631997\n",
      "Epoch 75, loss: 685.823927\n",
      "Epoch 76, loss: 683.071326\n",
      "Epoch 77, loss: 684.614091\n",
      "Epoch 78, loss: 684.425001\n",
      "Epoch 79, loss: 683.762786\n",
      "Epoch 80, loss: 685.007899\n",
      "Epoch 81, loss: 684.691027\n",
      "Epoch 82, loss: 684.891090\n",
      "Epoch 83, loss: 684.786310\n",
      "Epoch 84, loss: 685.532806\n",
      "Epoch 85, loss: 682.168302\n",
      "Epoch 86, loss: 686.581380\n",
      "Epoch 87, loss: 685.187499\n",
      "Epoch 88, loss: 682.973308\n",
      "Epoch 89, loss: 685.087607\n",
      "Epoch 90, loss: 685.592629\n",
      "Epoch 91, loss: 683.766868\n",
      "Epoch 92, loss: 683.435220\n",
      "Epoch 93, loss: 682.468257\n",
      "Epoch 94, loss: 682.987222\n",
      "Epoch 95, loss: 684.699512\n",
      "Epoch 96, loss: 684.940453\n",
      "Epoch 97, loss: 681.422125\n",
      "Epoch 98, loss: 682.851927\n",
      "Epoch 99, loss: 683.210957\n",
      "Epoch 100, loss: 685.432686\n",
      "Epoch 101, loss: 683.164626\n",
      "Epoch 102, loss: 682.638509\n",
      "Epoch 103, loss: 684.085005\n",
      "Epoch 104, loss: 682.954856\n",
      "Epoch 105, loss: 682.281153\n",
      "Epoch 106, loss: 683.232269\n",
      "Epoch 107, loss: 683.168769\n",
      "Epoch 108, loss: 681.908386\n",
      "Epoch 109, loss: 682.413551\n",
      "Epoch 110, loss: 684.186042\n",
      "Epoch 111, loss: 684.156653\n",
      "Epoch 112, loss: 682.983431\n",
      "Epoch 113, loss: 681.465730\n",
      "Epoch 114, loss: 679.139854\n",
      "Epoch 115, loss: 681.389410\n",
      "Epoch 116, loss: 683.394778\n",
      "Epoch 117, loss: 682.349289\n",
      "Epoch 118, loss: 682.288377\n",
      "Epoch 119, loss: 680.296884\n",
      "Epoch 120, loss: 681.895554\n",
      "Epoch 121, loss: 680.624017\n",
      "Epoch 122, loss: 681.122594\n",
      "Epoch 123, loss: 682.458250\n",
      "Epoch 124, loss: 681.598181\n",
      "Epoch 125, loss: 682.101441\n",
      "Epoch 126, loss: 681.812382\n",
      "Epoch 127, loss: 681.468702\n",
      "Epoch 128, loss: 683.054719\n",
      "Epoch 129, loss: 677.901869\n",
      "Epoch 130, loss: 679.408790\n",
      "Epoch 131, loss: 680.270002\n",
      "Epoch 132, loss: 683.472061\n",
      "Epoch 133, loss: 677.853173\n",
      "Epoch 134, loss: 682.518801\n",
      "Epoch 135, loss: 679.662792\n",
      "Epoch 136, loss: 680.270211\n",
      "Epoch 137, loss: 681.143047\n",
      "Epoch 138, loss: 679.338088\n",
      "Epoch 139, loss: 678.815713\n",
      "Epoch 140, loss: 680.073186\n",
      "Epoch 141, loss: 681.438413\n",
      "Epoch 142, loss: 677.285759\n",
      "Epoch 143, loss: 680.325526\n",
      "Epoch 144, loss: 680.327947\n",
      "Epoch 145, loss: 678.207777\n",
      "Epoch 146, loss: 679.224210\n",
      "Epoch 147, loss: 679.589982\n",
      "Epoch 148, loss: 679.898346\n",
      "Epoch 149, loss: 680.823267\n",
      "Epoch 150, loss: 678.570669\n",
      "Epoch 151, loss: 678.938414\n",
      "Epoch 152, loss: 678.233412\n",
      "Epoch 153, loss: 681.239497\n",
      "Epoch 154, loss: 680.258638\n",
      "Epoch 155, loss: 681.896692\n",
      "Epoch 156, loss: 682.563004\n",
      "Epoch 157, loss: 675.764779\n",
      "Epoch 158, loss: 678.694688\n",
      "Epoch 159, loss: 680.676414\n",
      "Epoch 160, loss: 679.355934\n",
      "Epoch 161, loss: 679.606917\n",
      "Epoch 162, loss: 681.679846\n",
      "Epoch 163, loss: 677.346811\n",
      "Epoch 164, loss: 678.861514\n",
      "Epoch 165, loss: 679.911130\n",
      "Epoch 166, loss: 678.951080\n",
      "Epoch 167, loss: 683.280655\n",
      "Epoch 168, loss: 678.010554\n",
      "Epoch 169, loss: 680.165746\n",
      "Epoch 170, loss: 676.750294\n",
      "Epoch 171, loss: 675.930914\n",
      "Epoch 172, loss: 680.384609\n",
      "Epoch 173, loss: 681.461279\n",
      "Epoch 174, loss: 679.986234\n",
      "Epoch 175, loss: 681.102966\n",
      "Epoch 176, loss: 679.808110\n",
      "Epoch 177, loss: 680.202587\n",
      "Epoch 178, loss: 680.354365\n",
      "Epoch 179, loss: 675.974191\n",
      "Epoch 180, loss: 678.108666\n",
      "Epoch 181, loss: 678.221660\n",
      "Epoch 182, loss: 676.210274\n",
      "Epoch 183, loss: 678.754653\n",
      "Epoch 184, loss: 675.569139\n",
      "Epoch 185, loss: 677.496289\n",
      "Epoch 186, loss: 677.836420\n",
      "Epoch 187, loss: 678.174393\n",
      "Epoch 188, loss: 675.890440\n",
      "Epoch 189, loss: 678.944777\n",
      "Epoch 190, loss: 679.217134\n",
      "Epoch 191, loss: 678.833234\n",
      "Epoch 192, loss: 676.817151\n",
      "Epoch 193, loss: 679.528305\n",
      "Epoch 194, loss: 678.657484\n",
      "Epoch 195, loss: 675.676571\n",
      "Epoch 196, loss: 679.321497\n",
      "Epoch 197, loss: 673.470146\n",
      "Epoch 198, loss: 674.303358\n",
      "Epoch 199, loss: 674.917791\n",
      "Validation accuracy:  0.21 lr:  1e-06 reg:  0.001\n",
      "Epoch 0, loss: 690.514077\n",
      "Epoch 1, loss: 688.963235\n",
      "Epoch 2, loss: 689.178210\n",
      "Epoch 3, loss: 688.229440\n",
      "Epoch 4, loss: 688.343070\n",
      "Epoch 5, loss: 686.945912\n",
      "Epoch 6, loss: 683.784533\n",
      "Epoch 7, loss: 684.734375\n",
      "Epoch 8, loss: 683.573916\n",
      "Epoch 9, loss: 683.342089\n",
      "Epoch 10, loss: 683.506658\n",
      "Epoch 11, loss: 683.966713\n",
      "Epoch 12, loss: 683.301372\n",
      "Epoch 13, loss: 681.925589\n",
      "Epoch 14, loss: 679.934473\n",
      "Epoch 15, loss: 679.514263\n",
      "Epoch 16, loss: 674.972682\n",
      "Epoch 17, loss: 680.286811\n",
      "Epoch 18, loss: 678.710090\n",
      "Epoch 19, loss: 677.206276\n",
      "Epoch 20, loss: 676.357885\n",
      "Epoch 21, loss: 677.430107\n",
      "Epoch 22, loss: 675.685151\n",
      "Epoch 23, loss: 674.501645\n",
      "Epoch 24, loss: 674.991065\n",
      "Epoch 25, loss: 674.393450\n",
      "Epoch 26, loss: 673.486634\n",
      "Epoch 27, loss: 675.608158\n",
      "Epoch 28, loss: 676.006711\n",
      "Epoch 29, loss: 670.384747\n",
      "Epoch 30, loss: 672.847174\n",
      "Epoch 31, loss: 668.943942\n",
      "Epoch 32, loss: 669.131368\n",
      "Epoch 33, loss: 670.480494\n",
      "Epoch 34, loss: 668.539985\n",
      "Epoch 35, loss: 665.777018\n",
      "Epoch 36, loss: 667.463725\n",
      "Epoch 37, loss: 671.537599\n",
      "Epoch 38, loss: 672.911456\n",
      "Epoch 39, loss: 670.868305\n",
      "Epoch 40, loss: 667.457377\n",
      "Epoch 41, loss: 669.351248\n",
      "Epoch 42, loss: 673.810921\n",
      "Epoch 43, loss: 673.135907\n",
      "Epoch 44, loss: 673.646043\n",
      "Epoch 45, loss: 666.959471\n",
      "Epoch 46, loss: 669.266759\n",
      "Epoch 47, loss: 661.506274\n",
      "Epoch 48, loss: 670.645278\n",
      "Epoch 49, loss: 670.209401\n",
      "Epoch 50, loss: 661.786302\n",
      "Epoch 51, loss: 663.899762\n",
      "Epoch 52, loss: 660.223406\n",
      "Epoch 53, loss: 662.062294\n",
      "Epoch 54, loss: 660.997262\n",
      "Epoch 55, loss: 655.206267\n",
      "Epoch 56, loss: 658.661938\n",
      "Epoch 57, loss: 663.141621\n",
      "Epoch 58, loss: 668.956740\n",
      "Epoch 59, loss: 659.393641\n",
      "Epoch 60, loss: 659.786739\n",
      "Epoch 61, loss: 668.921921\n",
      "Epoch 62, loss: 661.347815\n",
      "Epoch 63, loss: 660.694887\n",
      "Epoch 64, loss: 665.171121\n",
      "Epoch 65, loss: 668.595675\n",
      "Epoch 66, loss: 659.313650\n",
      "Epoch 67, loss: 661.805344\n",
      "Epoch 68, loss: 659.779731\n",
      "Epoch 69, loss: 655.230374\n",
      "Epoch 70, loss: 659.968327\n",
      "Epoch 71, loss: 652.156086\n",
      "Epoch 72, loss: 654.659594\n",
      "Epoch 73, loss: 655.174192\n",
      "Epoch 74, loss: 666.684658\n",
      "Epoch 75, loss: 658.130377\n",
      "Epoch 76, loss: 653.825737\n",
      "Epoch 77, loss: 665.205929\n",
      "Epoch 78, loss: 655.403995\n",
      "Epoch 79, loss: 657.076947\n",
      "Epoch 80, loss: 664.296514\n",
      "Epoch 81, loss: 658.761076\n",
      "Epoch 82, loss: 661.920959\n",
      "Epoch 83, loss: 651.084051\n",
      "Epoch 84, loss: 659.928839\n",
      "Epoch 85, loss: 647.087289\n",
      "Epoch 86, loss: 659.541468\n",
      "Epoch 87, loss: 645.436552\n",
      "Epoch 88, loss: 652.579248\n",
      "Epoch 89, loss: 657.803687\n",
      "Epoch 90, loss: 662.470354\n",
      "Epoch 91, loss: 650.846866\n",
      "Epoch 92, loss: 659.811233\n",
      "Epoch 93, loss: 650.886203\n",
      "Epoch 94, loss: 646.631513\n",
      "Epoch 95, loss: 650.358647\n",
      "Epoch 96, loss: 659.365744\n",
      "Epoch 97, loss: 664.730464\n",
      "Epoch 98, loss: 650.658001\n",
      "Epoch 99, loss: 647.832525\n",
      "Epoch 100, loss: 653.725192\n",
      "Epoch 101, loss: 639.395153\n",
      "Epoch 102, loss: 653.509281\n",
      "Epoch 103, loss: 659.858768\n",
      "Epoch 104, loss: 644.546948\n",
      "Epoch 105, loss: 649.429633\n",
      "Epoch 106, loss: 664.668234\n",
      "Epoch 107, loss: 652.814635\n",
      "Epoch 108, loss: 652.002398\n",
      "Epoch 109, loss: 654.065889\n",
      "Epoch 110, loss: 657.537209\n",
      "Epoch 111, loss: 655.224340\n",
      "Epoch 112, loss: 642.777102\n",
      "Epoch 113, loss: 644.012177\n",
      "Epoch 114, loss: 653.751694\n",
      "Epoch 115, loss: 646.664650\n",
      "Epoch 116, loss: 648.620776\n",
      "Epoch 117, loss: 660.832975\n",
      "Epoch 118, loss: 657.407206\n",
      "Epoch 119, loss: 651.285801\n",
      "Epoch 120, loss: 653.078370\n",
      "Epoch 121, loss: 638.528924\n",
      "Epoch 122, loss: 658.806649\n",
      "Epoch 123, loss: 653.618139\n",
      "Epoch 124, loss: 653.557687\n",
      "Epoch 125, loss: 647.117356\n",
      "Epoch 126, loss: 654.668256\n",
      "Epoch 127, loss: 647.006974\n",
      "Epoch 128, loss: 657.102333\n",
      "Epoch 129, loss: 659.237573\n",
      "Epoch 130, loss: 644.279247\n",
      "Epoch 131, loss: 650.449639\n",
      "Epoch 132, loss: 645.453252\n",
      "Epoch 133, loss: 650.008628\n",
      "Epoch 134, loss: 671.021224\n",
      "Epoch 135, loss: 638.956130\n",
      "Epoch 136, loss: 661.822022\n",
      "Epoch 137, loss: 647.100871\n",
      "Epoch 138, loss: 646.262226\n",
      "Epoch 139, loss: 657.836311\n",
      "Epoch 140, loss: 640.031591\n",
      "Epoch 141, loss: 660.404807\n",
      "Epoch 142, loss: 647.211198\n",
      "Epoch 143, loss: 655.188742\n",
      "Epoch 144, loss: 652.423895\n",
      "Epoch 145, loss: 656.154896\n",
      "Epoch 146, loss: 637.273168\n",
      "Epoch 147, loss: 646.947837\n",
      "Epoch 148, loss: 646.848940\n",
      "Epoch 149, loss: 656.773307\n",
      "Epoch 150, loss: 635.990971\n",
      "Epoch 151, loss: 646.216246\n",
      "Epoch 152, loss: 642.064682\n",
      "Epoch 153, loss: 645.713288\n",
      "Epoch 154, loss: 642.054807\n",
      "Epoch 155, loss: 650.911928\n",
      "Epoch 156, loss: 643.205428\n",
      "Epoch 157, loss: 641.356485\n",
      "Epoch 158, loss: 648.377213\n",
      "Epoch 159, loss: 642.255114\n",
      "Epoch 160, loss: 638.878034\n",
      "Epoch 161, loss: 648.189522\n",
      "Epoch 162, loss: 656.970840\n",
      "Epoch 163, loss: 649.435027\n",
      "Epoch 164, loss: 650.240205\n",
      "Epoch 165, loss: 651.061796\n",
      "Epoch 166, loss: 649.321912\n",
      "Epoch 167, loss: 645.921368\n",
      "Epoch 168, loss: 663.976851\n",
      "Epoch 169, loss: 648.138900\n",
      "Epoch 170, loss: 658.292581\n",
      "Epoch 171, loss: 647.938170\n",
      "Epoch 172, loss: 646.245811\n",
      "Epoch 173, loss: 651.552632\n",
      "Epoch 174, loss: 629.913301\n",
      "Epoch 175, loss: 649.636564\n",
      "Epoch 176, loss: 641.333021\n",
      "Epoch 177, loss: 640.461232\n",
      "Epoch 178, loss: 660.342323\n",
      "Epoch 179, loss: 655.614940\n",
      "Epoch 180, loss: 652.876759\n",
      "Epoch 181, loss: 636.846667\n",
      "Epoch 182, loss: 648.441616\n",
      "Epoch 183, loss: 652.153341\n",
      "Epoch 184, loss: 637.358033\n",
      "Epoch 185, loss: 641.423403\n",
      "Epoch 186, loss: 642.661814\n",
      "Epoch 187, loss: 642.910149\n",
      "Epoch 188, loss: 656.676430\n",
      "Epoch 189, loss: 646.758613\n",
      "Epoch 190, loss: 649.806116\n",
      "Epoch 191, loss: 640.979920\n",
      "Epoch 192, loss: 638.582765\n",
      "Epoch 193, loss: 640.207819\n",
      "Epoch 194, loss: 642.330510\n",
      "Epoch 195, loss: 645.264455\n",
      "Epoch 196, loss: 642.883117\n",
      "Epoch 197, loss: 643.022596\n",
      "Epoch 198, loss: 651.286215\n",
      "Epoch 199, loss: 646.805896\n",
      "Validation accuracy:  0.228 lr:  1e-05 reg:  1e-06\n",
      "Epoch 0, loss: 690.158691\n",
      "Epoch 1, loss: 689.420372\n",
      "Epoch 2, loss: 688.713080\n",
      "Epoch 3, loss: 688.179824\n",
      "Epoch 4, loss: 687.687754\n",
      "Epoch 5, loss: 686.661363\n",
      "Epoch 6, loss: 685.674015\n",
      "Epoch 7, loss: 684.000836\n",
      "Epoch 8, loss: 682.695473\n",
      "Epoch 9, loss: 683.620394\n",
      "Epoch 10, loss: 684.306148\n",
      "Epoch 11, loss: 683.292975\n",
      "Epoch 12, loss: 681.868220\n",
      "Epoch 13, loss: 682.829165\n",
      "Epoch 14, loss: 680.446649\n",
      "Epoch 15, loss: 680.135966\n",
      "Epoch 16, loss: 679.143858\n",
      "Epoch 17, loss: 679.235488\n",
      "Epoch 18, loss: 677.256506\n",
      "Epoch 19, loss: 675.810711\n",
      "Epoch 20, loss: 676.843324\n",
      "Epoch 21, loss: 673.079896\n",
      "Epoch 22, loss: 677.785192\n",
      "Epoch 23, loss: 674.896444\n",
      "Epoch 24, loss: 678.908763\n",
      "Epoch 25, loss: 673.130610\n",
      "Epoch 26, loss: 674.748910\n",
      "Epoch 27, loss: 673.765999\n",
      "Epoch 28, loss: 672.613488\n",
      "Epoch 29, loss: 674.987532\n",
      "Epoch 30, loss: 672.849162\n",
      "Epoch 31, loss: 672.097439\n",
      "Epoch 32, loss: 669.613499\n",
      "Epoch 33, loss: 672.631195\n",
      "Epoch 34, loss: 667.118066\n",
      "Epoch 35, loss: 664.820760\n",
      "Epoch 36, loss: 675.098306\n",
      "Epoch 37, loss: 667.762892\n",
      "Epoch 38, loss: 669.176223\n",
      "Epoch 39, loss: 667.235623\n",
      "Epoch 40, loss: 665.195592\n",
      "Epoch 41, loss: 668.923567\n",
      "Epoch 42, loss: 664.120179\n",
      "Epoch 43, loss: 674.312257\n",
      "Epoch 44, loss: 674.825268\n",
      "Epoch 45, loss: 670.440487\n",
      "Epoch 46, loss: 671.413688\n",
      "Epoch 47, loss: 666.133727\n",
      "Epoch 48, loss: 664.414622\n",
      "Epoch 49, loss: 667.692003\n",
      "Epoch 50, loss: 663.807448\n",
      "Epoch 51, loss: 659.606383\n",
      "Epoch 52, loss: 666.199176\n",
      "Epoch 53, loss: 658.336238\n",
      "Epoch 54, loss: 667.160250\n",
      "Epoch 55, loss: 654.349945\n",
      "Epoch 56, loss: 665.227600\n",
      "Epoch 57, loss: 664.537255\n",
      "Epoch 58, loss: 667.983538\n",
      "Epoch 59, loss: 665.397565\n",
      "Epoch 60, loss: 664.453578\n",
      "Epoch 61, loss: 661.029784\n",
      "Epoch 62, loss: 659.094964\n",
      "Epoch 63, loss: 663.318512\n",
      "Epoch 64, loss: 663.666935\n",
      "Epoch 65, loss: 663.028355\n",
      "Epoch 66, loss: 662.362765\n",
      "Epoch 67, loss: 659.618371\n",
      "Epoch 68, loss: 657.057151\n",
      "Epoch 69, loss: 649.453165\n",
      "Epoch 70, loss: 666.282573\n",
      "Epoch 71, loss: 664.979622\n",
      "Epoch 72, loss: 654.706115\n",
      "Epoch 73, loss: 653.700853\n",
      "Epoch 74, loss: 660.648646\n",
      "Epoch 75, loss: 666.651086\n",
      "Epoch 76, loss: 652.552076\n",
      "Epoch 77, loss: 662.231812\n",
      "Epoch 78, loss: 652.999697\n",
      "Epoch 79, loss: 657.639810\n",
      "Epoch 80, loss: 659.562921\n",
      "Epoch 81, loss: 649.852443\n",
      "Epoch 82, loss: 657.435905\n",
      "Epoch 83, loss: 662.575802\n",
      "Epoch 84, loss: 666.592680\n",
      "Epoch 85, loss: 655.393336\n",
      "Epoch 86, loss: 651.308217\n",
      "Epoch 87, loss: 661.568014\n",
      "Epoch 88, loss: 661.456653\n",
      "Epoch 89, loss: 657.606212\n",
      "Epoch 90, loss: 662.131014\n",
      "Epoch 91, loss: 659.789537\n",
      "Epoch 92, loss: 650.897715\n",
      "Epoch 93, loss: 656.083587\n",
      "Epoch 94, loss: 651.860265\n",
      "Epoch 95, loss: 657.427276\n",
      "Epoch 96, loss: 649.805622\n",
      "Epoch 97, loss: 661.846638\n",
      "Epoch 98, loss: 654.115083\n",
      "Epoch 99, loss: 651.732637\n",
      "Epoch 100, loss: 660.317385\n",
      "Epoch 101, loss: 660.396681\n",
      "Epoch 102, loss: 654.079110\n",
      "Epoch 103, loss: 650.392294\n",
      "Epoch 104, loss: 650.311617\n",
      "Epoch 105, loss: 640.065328\n",
      "Epoch 106, loss: 640.348571\n",
      "Epoch 107, loss: 657.332383\n",
      "Epoch 108, loss: 650.103211\n",
      "Epoch 109, loss: 642.687392\n",
      "Epoch 110, loss: 654.949732\n",
      "Epoch 111, loss: 657.983012\n",
      "Epoch 112, loss: 656.174492\n",
      "Epoch 113, loss: 646.145821\n",
      "Epoch 114, loss: 654.668234\n",
      "Epoch 115, loss: 656.215099\n",
      "Epoch 116, loss: 661.867435\n",
      "Epoch 117, loss: 651.537871\n",
      "Epoch 118, loss: 654.492522\n",
      "Epoch 119, loss: 642.695888\n",
      "Epoch 120, loss: 647.242453\n",
      "Epoch 121, loss: 647.289267\n",
      "Epoch 122, loss: 639.416276\n",
      "Epoch 123, loss: 649.568730\n",
      "Epoch 124, loss: 648.631694\n",
      "Epoch 125, loss: 632.092107\n",
      "Epoch 126, loss: 655.726396\n",
      "Epoch 127, loss: 651.923507\n",
      "Epoch 128, loss: 640.898101\n",
      "Epoch 129, loss: 648.678664\n",
      "Epoch 130, loss: 653.834523\n",
      "Epoch 131, loss: 651.997052\n",
      "Epoch 132, loss: 638.733604\n",
      "Epoch 133, loss: 654.527519\n",
      "Epoch 134, loss: 654.011650\n",
      "Epoch 135, loss: 646.279810\n",
      "Epoch 136, loss: 647.110740\n",
      "Epoch 137, loss: 652.633488\n",
      "Epoch 138, loss: 645.484248\n",
      "Epoch 139, loss: 657.961774\n",
      "Epoch 140, loss: 646.127146\n",
      "Epoch 141, loss: 654.157051\n",
      "Epoch 142, loss: 644.029799\n",
      "Epoch 143, loss: 648.269494\n",
      "Epoch 144, loss: 639.807742\n",
      "Epoch 145, loss: 648.306456\n",
      "Epoch 146, loss: 642.403359\n",
      "Epoch 147, loss: 649.578997\n",
      "Epoch 148, loss: 641.682152\n",
      "Epoch 149, loss: 665.981345\n",
      "Epoch 150, loss: 654.299108\n",
      "Epoch 151, loss: 642.174262\n",
      "Epoch 152, loss: 649.245962\n",
      "Epoch 153, loss: 652.763240\n",
      "Epoch 154, loss: 640.754062\n",
      "Epoch 155, loss: 659.670812\n",
      "Epoch 156, loss: 641.561815\n",
      "Epoch 157, loss: 628.856067\n",
      "Epoch 158, loss: 658.811841\n",
      "Epoch 159, loss: 647.998637\n",
      "Epoch 160, loss: 651.455891\n",
      "Epoch 161, loss: 641.226261\n",
      "Epoch 162, loss: 639.380585\n",
      "Epoch 163, loss: 641.988936\n",
      "Epoch 164, loss: 636.653719\n",
      "Epoch 165, loss: 639.860240\n",
      "Epoch 166, loss: 640.880050\n",
      "Epoch 167, loss: 646.928559\n",
      "Epoch 168, loss: 648.201782\n",
      "Epoch 169, loss: 644.254668\n",
      "Epoch 170, loss: 651.403551\n",
      "Epoch 171, loss: 652.531094\n",
      "Epoch 172, loss: 658.259921\n",
      "Epoch 173, loss: 638.311273\n",
      "Epoch 174, loss: 636.603791\n",
      "Epoch 175, loss: 631.443484\n",
      "Epoch 176, loss: 652.320940\n",
      "Epoch 177, loss: 654.842726\n",
      "Epoch 178, loss: 650.013459\n",
      "Epoch 179, loss: 653.111715\n",
      "Epoch 180, loss: 641.193646\n",
      "Epoch 181, loss: 661.235420\n",
      "Epoch 182, loss: 631.334468\n",
      "Epoch 183, loss: 646.180025\n",
      "Epoch 184, loss: 643.028181\n",
      "Epoch 185, loss: 658.940936\n",
      "Epoch 186, loss: 649.717060\n",
      "Epoch 187, loss: 650.640663\n",
      "Epoch 188, loss: 648.262092\n",
      "Epoch 189, loss: 651.522538\n",
      "Epoch 190, loss: 643.993269\n",
      "Epoch 191, loss: 644.647376\n",
      "Epoch 192, loss: 642.828773\n",
      "Epoch 193, loss: 633.718213\n",
      "Epoch 194, loss: 662.555004\n",
      "Epoch 195, loss: 640.673343\n",
      "Epoch 196, loss: 633.098166\n",
      "Epoch 197, loss: 649.593403\n",
      "Epoch 198, loss: 646.078268\n",
      "Epoch 199, loss: 639.360654\n",
      "Validation accuracy:  0.228 lr:  1e-05 reg:  1e-05\n",
      "Epoch 0, loss: 690.282688\n",
      "Epoch 1, loss: 689.473006\n",
      "Epoch 2, loss: 687.649947\n",
      "Epoch 3, loss: 687.030912\n",
      "Epoch 4, loss: 687.131854\n",
      "Epoch 5, loss: 685.406885\n",
      "Epoch 6, loss: 685.473302\n",
      "Epoch 7, loss: 682.195167\n",
      "Epoch 8, loss: 684.800613\n",
      "Epoch 9, loss: 681.719912\n",
      "Epoch 10, loss: 684.748689\n",
      "Epoch 11, loss: 681.746062\n",
      "Epoch 12, loss: 684.300531\n",
      "Epoch 13, loss: 676.932745\n",
      "Epoch 14, loss: 677.537340\n",
      "Epoch 15, loss: 679.233443\n",
      "Epoch 16, loss: 678.528859\n",
      "Epoch 17, loss: 678.456905\n",
      "Epoch 18, loss: 678.462210\n",
      "Epoch 19, loss: 677.138957\n",
      "Epoch 20, loss: 676.291972\n",
      "Epoch 21, loss: 677.642650\n",
      "Epoch 22, loss: 676.906044\n",
      "Epoch 23, loss: 676.578523\n",
      "Epoch 24, loss: 681.982995\n",
      "Epoch 25, loss: 669.656070\n",
      "Epoch 26, loss: 672.440734\n",
      "Epoch 27, loss: 670.932679\n",
      "Epoch 28, loss: 673.116300\n",
      "Epoch 29, loss: 675.128953\n",
      "Epoch 30, loss: 672.458172\n",
      "Epoch 31, loss: 671.103294\n",
      "Epoch 32, loss: 672.117992\n",
      "Epoch 33, loss: 665.923211\n",
      "Epoch 34, loss: 671.491389\n",
      "Epoch 35, loss: 675.195238\n",
      "Epoch 36, loss: 674.365803\n",
      "Epoch 37, loss: 669.365978\n",
      "Epoch 38, loss: 668.231690\n",
      "Epoch 39, loss: 665.143126\n",
      "Epoch 40, loss: 668.911557\n",
      "Epoch 41, loss: 667.540075\n",
      "Epoch 42, loss: 669.466335\n",
      "Epoch 43, loss: 673.920783\n",
      "Epoch 44, loss: 669.199424\n",
      "Epoch 45, loss: 667.297717\n",
      "Epoch 46, loss: 671.408879\n",
      "Epoch 47, loss: 670.566497\n",
      "Epoch 48, loss: 664.729217\n",
      "Epoch 49, loss: 663.574775\n",
      "Epoch 50, loss: 668.439956\n",
      "Epoch 51, loss: 665.723837\n",
      "Epoch 52, loss: 665.003244\n",
      "Epoch 53, loss: 657.150655\n",
      "Epoch 54, loss: 662.824650\n",
      "Epoch 55, loss: 672.981957\n",
      "Epoch 56, loss: 660.093323\n",
      "Epoch 57, loss: 662.160646\n",
      "Epoch 58, loss: 664.610764\n",
      "Epoch 59, loss: 660.167271\n",
      "Epoch 60, loss: 659.892042\n",
      "Epoch 61, loss: 665.999597\n",
      "Epoch 62, loss: 666.765361\n",
      "Epoch 63, loss: 661.851182\n",
      "Epoch 64, loss: 663.513297\n",
      "Epoch 65, loss: 658.192419\n",
      "Epoch 66, loss: 660.556173\n",
      "Epoch 67, loss: 656.944108\n",
      "Epoch 68, loss: 654.858306\n",
      "Epoch 69, loss: 663.007922\n",
      "Epoch 70, loss: 663.501798\n",
      "Epoch 71, loss: 654.084488\n",
      "Epoch 72, loss: 667.620558\n",
      "Epoch 73, loss: 658.056196\n",
      "Epoch 74, loss: 661.920178\n",
      "Epoch 75, loss: 653.666236\n",
      "Epoch 76, loss: 651.423662\n",
      "Epoch 77, loss: 660.757615\n",
      "Epoch 78, loss: 651.750867\n",
      "Epoch 79, loss: 657.683029\n",
      "Epoch 80, loss: 657.517404\n",
      "Epoch 81, loss: 660.762222\n",
      "Epoch 82, loss: 644.296824\n",
      "Epoch 83, loss: 654.602449\n",
      "Epoch 84, loss: 662.572412\n",
      "Epoch 85, loss: 651.097366\n",
      "Epoch 86, loss: 657.215978\n",
      "Epoch 87, loss: 659.736556\n",
      "Epoch 88, loss: 648.143934\n",
      "Epoch 89, loss: 660.478255\n",
      "Epoch 90, loss: 666.048240\n",
      "Epoch 91, loss: 660.881825\n",
      "Epoch 92, loss: 654.547549\n",
      "Epoch 93, loss: 650.924554\n",
      "Epoch 94, loss: 665.461108\n",
      "Epoch 95, loss: 647.081705\n",
      "Epoch 96, loss: 653.182217\n",
      "Epoch 97, loss: 653.859166\n",
      "Epoch 98, loss: 658.766046\n",
      "Epoch 99, loss: 649.895880\n",
      "Epoch 100, loss: 651.896605\n",
      "Epoch 101, loss: 655.368522\n",
      "Epoch 102, loss: 656.219487\n",
      "Epoch 103, loss: 659.190385\n",
      "Epoch 104, loss: 651.229482\n",
      "Epoch 105, loss: 655.105713\n",
      "Epoch 106, loss: 643.694054\n",
      "Epoch 107, loss: 653.800421\n",
      "Epoch 108, loss: 649.475571\n",
      "Epoch 109, loss: 655.256591\n",
      "Epoch 110, loss: 660.659047\n",
      "Epoch 111, loss: 666.460787\n",
      "Epoch 112, loss: 661.160644\n",
      "Epoch 113, loss: 651.124844\n",
      "Epoch 114, loss: 642.590605\n",
      "Epoch 115, loss: 649.441798\n",
      "Epoch 116, loss: 648.643991\n",
      "Epoch 117, loss: 644.852419\n",
      "Epoch 118, loss: 650.973235\n",
      "Epoch 119, loss: 650.358627\n",
      "Epoch 120, loss: 645.372571\n",
      "Epoch 121, loss: 656.404636\n",
      "Epoch 122, loss: 647.862511\n",
      "Epoch 123, loss: 657.038560\n",
      "Epoch 124, loss: 656.149523\n",
      "Epoch 125, loss: 638.141585\n",
      "Epoch 126, loss: 632.815142\n",
      "Epoch 127, loss: 644.918549\n",
      "Epoch 128, loss: 642.567903\n",
      "Epoch 129, loss: 639.494518\n",
      "Epoch 130, loss: 648.445556\n",
      "Epoch 131, loss: 657.189787\n",
      "Epoch 132, loss: 651.650547\n",
      "Epoch 133, loss: 675.034432\n",
      "Epoch 134, loss: 661.448588\n",
      "Epoch 135, loss: 651.497999\n",
      "Epoch 136, loss: 655.361363\n",
      "Epoch 137, loss: 648.274378\n",
      "Epoch 138, loss: 639.934152\n",
      "Epoch 139, loss: 659.002774\n",
      "Epoch 140, loss: 648.248556\n",
      "Epoch 141, loss: 650.186922\n",
      "Epoch 142, loss: 649.960883\n",
      "Epoch 143, loss: 646.803244\n",
      "Epoch 144, loss: 653.906711\n",
      "Epoch 145, loss: 642.749854\n",
      "Epoch 146, loss: 650.813384\n",
      "Epoch 147, loss: 648.987130\n",
      "Epoch 148, loss: 643.429098\n",
      "Epoch 149, loss: 645.165731\n",
      "Epoch 150, loss: 647.497005\n",
      "Epoch 151, loss: 659.806169\n",
      "Epoch 152, loss: 646.335117\n",
      "Epoch 153, loss: 644.688378\n",
      "Epoch 154, loss: 641.970623\n",
      "Epoch 155, loss: 650.340063\n",
      "Epoch 156, loss: 648.803472\n",
      "Epoch 157, loss: 652.134687\n",
      "Epoch 158, loss: 645.458757\n",
      "Epoch 159, loss: 652.793998\n",
      "Epoch 160, loss: 650.251658\n",
      "Epoch 161, loss: 656.400872\n",
      "Epoch 162, loss: 646.654273\n",
      "Epoch 163, loss: 648.418890\n",
      "Epoch 164, loss: 648.602230\n",
      "Epoch 165, loss: 653.665727\n",
      "Epoch 166, loss: 642.191771\n",
      "Epoch 167, loss: 644.922474\n",
      "Epoch 168, loss: 662.226992\n",
      "Epoch 169, loss: 640.587465\n",
      "Epoch 170, loss: 648.991072\n",
      "Epoch 171, loss: 651.969260\n",
      "Epoch 172, loss: 658.006091\n",
      "Epoch 173, loss: 644.802378\n",
      "Epoch 174, loss: 643.672982\n",
      "Epoch 175, loss: 636.574840\n",
      "Epoch 176, loss: 641.180082\n",
      "Epoch 177, loss: 646.422771\n",
      "Epoch 178, loss: 643.429606\n",
      "Epoch 179, loss: 655.371156\n",
      "Epoch 180, loss: 642.140521\n",
      "Epoch 181, loss: 640.254690\n",
      "Epoch 182, loss: 645.507936\n",
      "Epoch 183, loss: 649.976699\n",
      "Epoch 184, loss: 651.583288\n",
      "Epoch 185, loss: 644.623938\n",
      "Epoch 186, loss: 646.887067\n",
      "Epoch 187, loss: 638.551858\n",
      "Epoch 188, loss: 649.256136\n",
      "Epoch 189, loss: 636.749919\n",
      "Epoch 190, loss: 637.016841\n",
      "Epoch 191, loss: 652.570806\n",
      "Epoch 192, loss: 631.727646\n",
      "Epoch 193, loss: 654.008705\n",
      "Epoch 194, loss: 641.289003\n",
      "Epoch 195, loss: 640.582856\n",
      "Epoch 196, loss: 645.725813\n",
      "Epoch 197, loss: 649.567076\n",
      "Epoch 198, loss: 646.142970\n",
      "Epoch 199, loss: 643.408752\n",
      "Validation accuracy:  0.229 lr:  1e-05 reg:  0.0001\n",
      "Epoch 0, loss: 689.885629\n",
      "Epoch 1, loss: 690.731162\n",
      "Epoch 2, loss: 687.805137\n",
      "Epoch 3, loss: 687.407848\n",
      "Epoch 4, loss: 687.230368\n",
      "Epoch 5, loss: 687.170178\n",
      "Epoch 6, loss: 684.870676\n",
      "Epoch 7, loss: 684.955552\n",
      "Epoch 8, loss: 683.820199\n",
      "Epoch 9, loss: 683.468388\n",
      "Epoch 10, loss: 683.408735\n",
      "Epoch 11, loss: 682.604046\n",
      "Epoch 12, loss: 681.079453\n",
      "Epoch 13, loss: 680.906070\n",
      "Epoch 14, loss: 679.780879\n",
      "Epoch 15, loss: 680.928802\n",
      "Epoch 16, loss: 681.442450\n",
      "Epoch 17, loss: 681.209663\n",
      "Epoch 18, loss: 677.960495\n",
      "Epoch 19, loss: 678.536978\n",
      "Epoch 20, loss: 675.667318\n",
      "Epoch 21, loss: 678.670110\n",
      "Epoch 22, loss: 681.053031\n",
      "Epoch 23, loss: 677.040085\n",
      "Epoch 24, loss: 674.208467\n",
      "Epoch 25, loss: 677.098578\n",
      "Epoch 26, loss: 676.877839\n",
      "Epoch 27, loss: 675.382761\n",
      "Epoch 28, loss: 676.014641\n",
      "Epoch 29, loss: 672.157113\n",
      "Epoch 30, loss: 675.442154\n",
      "Epoch 31, loss: 672.468477\n",
      "Epoch 32, loss: 669.274832\n",
      "Epoch 33, loss: 669.150789\n",
      "Epoch 34, loss: 668.015161\n",
      "Epoch 35, loss: 668.119790\n",
      "Epoch 36, loss: 672.881119\n",
      "Epoch 37, loss: 668.342991\n",
      "Epoch 38, loss: 670.495723\n",
      "Epoch 39, loss: 673.259313\n",
      "Epoch 40, loss: 662.862467\n",
      "Epoch 41, loss: 663.474316\n",
      "Epoch 42, loss: 666.795369\n",
      "Epoch 43, loss: 672.645743\n",
      "Epoch 44, loss: 660.669829\n",
      "Epoch 45, loss: 668.739151\n",
      "Epoch 46, loss: 671.460327\n",
      "Epoch 47, loss: 660.579356\n",
      "Epoch 48, loss: 665.451479\n",
      "Epoch 49, loss: 666.355166\n",
      "Epoch 50, loss: 666.841839\n",
      "Epoch 51, loss: 663.451575\n",
      "Epoch 52, loss: 659.705218\n",
      "Epoch 53, loss: 663.659808\n",
      "Epoch 54, loss: 670.751944\n",
      "Epoch 55, loss: 665.120953\n",
      "Epoch 56, loss: 663.181717\n",
      "Epoch 57, loss: 655.387483\n",
      "Epoch 58, loss: 662.343765\n",
      "Epoch 59, loss: 661.371212\n",
      "Epoch 60, loss: 655.833671\n",
      "Epoch 61, loss: 658.924272\n",
      "Epoch 62, loss: 663.274155\n",
      "Epoch 63, loss: 668.927542\n",
      "Epoch 64, loss: 662.143742\n",
      "Epoch 65, loss: 661.594836\n",
      "Epoch 66, loss: 660.442935\n",
      "Epoch 67, loss: 656.667294\n",
      "Epoch 68, loss: 658.241083\n",
      "Epoch 69, loss: 666.494157\n",
      "Epoch 70, loss: 658.228001\n",
      "Epoch 71, loss: 656.541085\n",
      "Epoch 72, loss: 659.463646\n",
      "Epoch 73, loss: 654.465218\n",
      "Epoch 74, loss: 658.973933\n",
      "Epoch 75, loss: 667.446602\n",
      "Epoch 76, loss: 660.296790\n",
      "Epoch 77, loss: 661.113615\n",
      "Epoch 78, loss: 652.296170\n",
      "Epoch 79, loss: 661.225002\n",
      "Epoch 80, loss: 658.040968\n",
      "Epoch 81, loss: 660.041571\n",
      "Epoch 82, loss: 657.501762\n",
      "Epoch 83, loss: 656.938533\n",
      "Epoch 84, loss: 652.290460\n",
      "Epoch 85, loss: 654.645568\n",
      "Epoch 86, loss: 649.514454\n",
      "Epoch 87, loss: 656.187289\n",
      "Epoch 88, loss: 649.259719\n",
      "Epoch 89, loss: 652.078418\n",
      "Epoch 90, loss: 658.999220\n",
      "Epoch 91, loss: 650.198557\n",
      "Epoch 92, loss: 656.771547\n",
      "Epoch 93, loss: 660.593130\n",
      "Epoch 94, loss: 659.627057\n",
      "Epoch 95, loss: 662.142647\n",
      "Epoch 96, loss: 654.123975\n",
      "Epoch 97, loss: 650.163302\n",
      "Epoch 98, loss: 647.283270\n",
      "Epoch 99, loss: 661.009175\n",
      "Epoch 100, loss: 659.322401\n",
      "Epoch 101, loss: 660.127940\n",
      "Epoch 102, loss: 650.876157\n",
      "Epoch 103, loss: 657.425067\n",
      "Epoch 104, loss: 653.372411\n",
      "Epoch 105, loss: 649.537217\n",
      "Epoch 106, loss: 655.451746\n",
      "Epoch 107, loss: 664.748544\n",
      "Epoch 108, loss: 651.581911\n",
      "Epoch 109, loss: 658.969164\n",
      "Epoch 110, loss: 647.125897\n",
      "Epoch 111, loss: 665.350250\n",
      "Epoch 112, loss: 655.429295\n",
      "Epoch 113, loss: 646.021270\n",
      "Epoch 114, loss: 659.917899\n",
      "Epoch 115, loss: 650.105387\n",
      "Epoch 116, loss: 647.674381\n",
      "Epoch 117, loss: 659.507117\n",
      "Epoch 118, loss: 647.909288\n",
      "Epoch 119, loss: 655.048461\n",
      "Epoch 120, loss: 652.606578\n",
      "Epoch 121, loss: 652.522391\n",
      "Epoch 122, loss: 655.268851\n",
      "Epoch 123, loss: 652.261640\n",
      "Epoch 124, loss: 650.639834\n",
      "Epoch 125, loss: 644.043566\n",
      "Epoch 126, loss: 650.264235\n",
      "Epoch 127, loss: 656.429245\n",
      "Epoch 128, loss: 653.197745\n",
      "Epoch 129, loss: 659.594425\n",
      "Epoch 130, loss: 644.990610\n",
      "Epoch 131, loss: 656.764735\n",
      "Epoch 132, loss: 655.204255\n",
      "Epoch 133, loss: 654.748464\n",
      "Epoch 134, loss: 653.638376\n",
      "Epoch 135, loss: 657.626966\n",
      "Epoch 136, loss: 655.554583\n",
      "Epoch 137, loss: 653.312094\n",
      "Epoch 138, loss: 652.859969\n",
      "Epoch 139, loss: 647.321616\n",
      "Epoch 140, loss: 646.522091\n",
      "Epoch 141, loss: 646.554677\n",
      "Epoch 142, loss: 649.618933\n",
      "Epoch 143, loss: 651.456895\n",
      "Epoch 144, loss: 645.283622\n",
      "Epoch 145, loss: 646.787213\n",
      "Epoch 146, loss: 644.391784\n",
      "Epoch 147, loss: 650.116850\n",
      "Epoch 148, loss: 641.727419\n",
      "Epoch 149, loss: 640.544518\n",
      "Epoch 150, loss: 654.098449\n",
      "Epoch 151, loss: 645.361922\n",
      "Epoch 152, loss: 652.517372\n",
      "Epoch 153, loss: 648.627802\n",
      "Epoch 154, loss: 655.101061\n",
      "Epoch 155, loss: 661.027571\n",
      "Epoch 156, loss: 657.112336\n",
      "Epoch 157, loss: 657.207894\n",
      "Epoch 158, loss: 643.108098\n",
      "Epoch 159, loss: 642.407796\n",
      "Epoch 160, loss: 654.104953\n",
      "Epoch 161, loss: 641.487759\n",
      "Epoch 162, loss: 643.403105\n",
      "Epoch 163, loss: 635.697653\n",
      "Epoch 164, loss: 654.062331\n",
      "Epoch 165, loss: 647.817433\n",
      "Epoch 166, loss: 650.524465\n",
      "Epoch 167, loss: 644.051708\n",
      "Epoch 168, loss: 646.690955\n",
      "Epoch 169, loss: 637.195377\n",
      "Epoch 170, loss: 655.871348\n",
      "Epoch 171, loss: 640.408718\n",
      "Epoch 172, loss: 648.499144\n",
      "Epoch 173, loss: 645.395029\n",
      "Epoch 174, loss: 635.740396\n",
      "Epoch 175, loss: 647.512988\n",
      "Epoch 176, loss: 637.349605\n",
      "Epoch 177, loss: 649.838774\n",
      "Epoch 178, loss: 650.948232\n",
      "Epoch 179, loss: 651.500867\n",
      "Epoch 180, loss: 642.076687\n",
      "Epoch 181, loss: 654.104076\n",
      "Epoch 182, loss: 633.203553\n",
      "Epoch 183, loss: 646.940022\n",
      "Epoch 184, loss: 637.692945\n",
      "Epoch 185, loss: 645.184876\n",
      "Epoch 186, loss: 649.229409\n",
      "Epoch 187, loss: 658.644684\n",
      "Epoch 188, loss: 645.988582\n",
      "Epoch 189, loss: 648.213031\n",
      "Epoch 190, loss: 664.967626\n",
      "Epoch 191, loss: 649.772544\n",
      "Epoch 192, loss: 641.769453\n",
      "Epoch 193, loss: 639.861269\n",
      "Epoch 194, loss: 650.940782\n",
      "Epoch 195, loss: 640.479120\n",
      "Epoch 196, loss: 637.234987\n",
      "Epoch 197, loss: 638.563861\n",
      "Epoch 198, loss: 652.959009\n",
      "Epoch 199, loss: 630.768467\n",
      "Validation accuracy:  0.231 lr:  1e-05 reg:  0.001\n",
      "Epoch 0, loss: 683.213141\n",
      "Epoch 1, loss: 678.589230\n",
      "Epoch 2, loss: 678.658493\n",
      "Epoch 3, loss: 672.205413\n",
      "Epoch 4, loss: 665.732981\n",
      "Epoch 5, loss: 665.715682\n",
      "Epoch 6, loss: 661.933417\n",
      "Epoch 7, loss: 655.341105\n",
      "Epoch 8, loss: 650.516202\n",
      "Epoch 9, loss: 654.327916\n",
      "Epoch 10, loss: 642.737291\n",
      "Epoch 11, loss: 655.460081\n",
      "Epoch 12, loss: 650.890630\n",
      "Epoch 13, loss: 646.638221\n",
      "Epoch 14, loss: 648.065384\n",
      "Epoch 15, loss: 643.234586\n",
      "Epoch 16, loss: 644.196863\n",
      "Epoch 17, loss: 632.383447\n",
      "Epoch 18, loss: 665.695280\n",
      "Epoch 19, loss: 636.072528\n",
      "Epoch 20, loss: 649.037833\n",
      "Epoch 21, loss: 648.507650\n",
      "Epoch 22, loss: 644.590743\n",
      "Epoch 23, loss: 653.444404\n",
      "Epoch 24, loss: 640.506110\n",
      "Epoch 25, loss: 643.479970\n",
      "Epoch 26, loss: 645.286337\n",
      "Epoch 27, loss: 632.540862\n",
      "Epoch 28, loss: 632.564323\n",
      "Epoch 29, loss: 654.499955\n",
      "Epoch 30, loss: 654.641569\n",
      "Epoch 31, loss: 635.414460\n",
      "Epoch 32, loss: 646.178057\n",
      "Epoch 33, loss: 640.580739\n",
      "Epoch 34, loss: 645.268507\n",
      "Epoch 35, loss: 638.465369\n",
      "Epoch 36, loss: 635.206260\n",
      "Epoch 37, loss: 653.334837\n",
      "Epoch 38, loss: 625.505419\n",
      "Epoch 39, loss: 629.854457\n",
      "Epoch 40, loss: 646.871870\n",
      "Epoch 41, loss: 634.600489\n",
      "Epoch 42, loss: 635.973461\n",
      "Epoch 43, loss: 630.864447\n",
      "Epoch 44, loss: 655.563371\n",
      "Epoch 45, loss: 632.433637\n",
      "Epoch 46, loss: 637.289191\n",
      "Epoch 47, loss: 624.890188\n",
      "Epoch 48, loss: 631.128117\n",
      "Epoch 49, loss: 649.266340\n",
      "Epoch 50, loss: 621.859208\n",
      "Epoch 51, loss: 628.294617\n",
      "Epoch 52, loss: 625.392358\n",
      "Epoch 53, loss: 629.585497\n",
      "Epoch 54, loss: 641.580901\n",
      "Epoch 55, loss: 627.228294\n",
      "Epoch 56, loss: 636.852127\n",
      "Epoch 57, loss: 616.771530\n",
      "Epoch 58, loss: 632.398400\n",
      "Epoch 59, loss: 621.409095\n",
      "Epoch 60, loss: 646.699847\n",
      "Epoch 61, loss: 639.703560\n",
      "Epoch 62, loss: 629.695364\n",
      "Epoch 63, loss: 624.711152\n",
      "Epoch 64, loss: 624.819578\n",
      "Epoch 65, loss: 640.544854\n",
      "Epoch 66, loss: 622.615559\n",
      "Epoch 67, loss: 644.923160\n",
      "Epoch 68, loss: 633.685400\n",
      "Epoch 69, loss: 629.000771\n",
      "Epoch 70, loss: 615.784253\n",
      "Epoch 71, loss: 642.088148\n",
      "Epoch 72, loss: 622.988855\n",
      "Epoch 73, loss: 631.368745\n",
      "Epoch 74, loss: 616.128233\n",
      "Epoch 75, loss: 647.340322\n",
      "Epoch 76, loss: 641.687048\n",
      "Epoch 77, loss: 628.043904\n",
      "Epoch 78, loss: 633.365238\n",
      "Epoch 79, loss: 631.587576\n",
      "Epoch 80, loss: 642.375332\n",
      "Epoch 81, loss: 637.234039\n",
      "Epoch 82, loss: 628.492215\n",
      "Epoch 83, loss: 635.652050\n",
      "Epoch 84, loss: 638.914959\n",
      "Epoch 85, loss: 631.879281\n",
      "Epoch 86, loss: 613.452818\n",
      "Epoch 87, loss: 618.376841\n",
      "Epoch 88, loss: 610.286567\n",
      "Epoch 89, loss: 613.576640\n",
      "Epoch 90, loss: 622.348435\n",
      "Epoch 91, loss: 630.716559\n",
      "Epoch 92, loss: 620.647433\n",
      "Epoch 93, loss: 623.388733\n",
      "Epoch 94, loss: 632.913876\n",
      "Epoch 95, loss: 617.323178\n",
      "Epoch 96, loss: 619.049060\n",
      "Epoch 97, loss: 614.706856\n",
      "Epoch 98, loss: 633.389577\n",
      "Epoch 99, loss: 612.247254\n",
      "Epoch 100, loss: 625.995180\n",
      "Epoch 101, loss: 608.834016\n",
      "Epoch 102, loss: 639.073538\n",
      "Epoch 103, loss: 634.471130\n",
      "Epoch 104, loss: 640.014542\n",
      "Epoch 105, loss: 623.626959\n",
      "Epoch 106, loss: 622.965122\n",
      "Epoch 107, loss: 629.141567\n",
      "Epoch 108, loss: 634.498687\n",
      "Epoch 109, loss: 624.592524\n",
      "Epoch 110, loss: 638.821001\n",
      "Epoch 111, loss: 613.989353\n",
      "Epoch 112, loss: 629.369972\n",
      "Epoch 113, loss: 625.264103\n",
      "Epoch 114, loss: 629.165360\n",
      "Epoch 115, loss: 617.352814\n",
      "Epoch 116, loss: 629.263756\n",
      "Epoch 117, loss: 629.195351\n",
      "Epoch 118, loss: 625.534143\n",
      "Epoch 119, loss: 634.353294\n",
      "Epoch 120, loss: 615.696322\n",
      "Epoch 121, loss: 629.240806\n",
      "Epoch 122, loss: 618.321776\n",
      "Epoch 123, loss: 633.015870\n",
      "Epoch 124, loss: 634.108269\n",
      "Epoch 125, loss: 613.753600\n",
      "Epoch 126, loss: 632.756037\n",
      "Epoch 127, loss: 631.467580\n",
      "Epoch 128, loss: 623.623516\n",
      "Epoch 129, loss: 628.099991\n",
      "Epoch 130, loss: 624.893413\n",
      "Epoch 131, loss: 625.180907\n",
      "Epoch 132, loss: 612.439782\n",
      "Epoch 133, loss: 633.883942\n",
      "Epoch 134, loss: 648.335275\n",
      "Epoch 135, loss: 633.902461\n",
      "Epoch 136, loss: 635.694937\n",
      "Epoch 137, loss: 595.383914\n",
      "Epoch 138, loss: 634.436765\n",
      "Epoch 139, loss: 613.553338\n",
      "Epoch 140, loss: 621.006379\n",
      "Epoch 141, loss: 623.986705\n",
      "Epoch 142, loss: 617.436072\n",
      "Epoch 143, loss: 612.437550\n",
      "Epoch 144, loss: 650.543662\n",
      "Epoch 145, loss: 638.204105\n",
      "Epoch 146, loss: 616.305682\n",
      "Epoch 147, loss: 617.923589\n",
      "Epoch 148, loss: 634.404285\n",
      "Epoch 149, loss: 631.522934\n",
      "Epoch 150, loss: 629.403814\n",
      "Epoch 151, loss: 638.220111\n",
      "Epoch 152, loss: 626.240672\n",
      "Epoch 153, loss: 618.082466\n",
      "Epoch 154, loss: 627.802375\n",
      "Epoch 155, loss: 620.626213\n",
      "Epoch 156, loss: 613.803652\n",
      "Epoch 157, loss: 620.425419\n",
      "Epoch 158, loss: 639.929645\n",
      "Epoch 159, loss: 627.770850\n",
      "Epoch 160, loss: 637.199061\n",
      "Epoch 161, loss: 622.208691\n",
      "Epoch 162, loss: 635.636200\n",
      "Epoch 163, loss: 626.398175\n",
      "Epoch 164, loss: 605.491966\n",
      "Epoch 165, loss: 648.627330\n",
      "Epoch 166, loss: 634.289202\n",
      "Epoch 167, loss: 601.581162\n",
      "Epoch 168, loss: 642.863307\n",
      "Epoch 169, loss: 636.837004\n",
      "Epoch 170, loss: 624.697986\n",
      "Epoch 171, loss: 618.513192\n",
      "Epoch 172, loss: 629.738077\n",
      "Epoch 173, loss: 614.319000\n",
      "Epoch 174, loss: 626.281014\n",
      "Epoch 175, loss: 610.940597\n",
      "Epoch 176, loss: 590.692920\n",
      "Epoch 177, loss: 618.241930\n",
      "Epoch 178, loss: 623.790902\n",
      "Epoch 179, loss: 618.585069\n",
      "Epoch 180, loss: 617.819586\n",
      "Epoch 181, loss: 601.532384\n",
      "Epoch 182, loss: 631.223126\n",
      "Epoch 183, loss: 635.726974\n",
      "Epoch 184, loss: 643.888729\n",
      "Epoch 185, loss: 619.442321\n",
      "Epoch 186, loss: 614.304061\n",
      "Epoch 187, loss: 625.736946\n",
      "Epoch 188, loss: 600.994460\n",
      "Epoch 189, loss: 602.854728\n",
      "Epoch 190, loss: 651.117286\n",
      "Epoch 191, loss: 594.423131\n",
      "Epoch 192, loss: 624.796268\n",
      "Epoch 193, loss: 618.066519\n",
      "Epoch 194, loss: 612.706834\n",
      "Epoch 195, loss: 629.287908\n",
      "Epoch 196, loss: 622.521998\n",
      "Epoch 197, loss: 626.603910\n",
      "Epoch 198, loss: 625.549017\n",
      "Epoch 199, loss: 605.316255\n",
      "Validation accuracy:  0.251 lr:  0.0001 reg:  1e-06\n",
      "Epoch 0, loss: 685.346921\n",
      "Epoch 1, loss: 679.302619\n",
      "Epoch 2, loss: 673.705150\n",
      "Epoch 3, loss: 671.333092\n",
      "Epoch 4, loss: 660.826807\n",
      "Epoch 5, loss: 662.550863\n",
      "Epoch 6, loss: 667.891422\n",
      "Epoch 7, loss: 659.945206\n",
      "Epoch 8, loss: 663.168443\n",
      "Epoch 9, loss: 659.896771\n",
      "Epoch 10, loss: 655.928501\n",
      "Epoch 11, loss: 658.872111\n",
      "Epoch 12, loss: 641.537698\n",
      "Epoch 13, loss: 646.429217\n",
      "Epoch 14, loss: 643.813836\n",
      "Epoch 15, loss: 646.778571\n",
      "Epoch 16, loss: 658.325543\n",
      "Epoch 17, loss: 643.963907\n",
      "Epoch 18, loss: 650.140918\n",
      "Epoch 19, loss: 646.540084\n",
      "Epoch 20, loss: 640.582246\n",
      "Epoch 21, loss: 639.419134\n",
      "Epoch 22, loss: 649.530995\n",
      "Epoch 23, loss: 631.595455\n",
      "Epoch 24, loss: 647.641331\n",
      "Epoch 25, loss: 635.790108\n",
      "Epoch 26, loss: 650.034879\n",
      "Epoch 27, loss: 642.641935\n",
      "Epoch 28, loss: 629.384865\n",
      "Epoch 29, loss: 632.039547\n",
      "Epoch 30, loss: 630.250347\n",
      "Epoch 31, loss: 644.704532\n",
      "Epoch 32, loss: 634.958417\n",
      "Epoch 33, loss: 656.679281\n",
      "Epoch 34, loss: 655.375227\n",
      "Epoch 35, loss: 644.923172\n",
      "Epoch 36, loss: 656.235969\n",
      "Epoch 37, loss: 632.336222\n",
      "Epoch 38, loss: 645.129805\n",
      "Epoch 39, loss: 664.883894\n",
      "Epoch 40, loss: 652.750637\n",
      "Epoch 41, loss: 631.558017\n",
      "Epoch 42, loss: 637.718146\n",
      "Epoch 43, loss: 650.296671\n",
      "Epoch 44, loss: 636.884410\n",
      "Epoch 45, loss: 626.334062\n",
      "Epoch 46, loss: 630.364342\n",
      "Epoch 47, loss: 638.236082\n",
      "Epoch 48, loss: 626.387011\n",
      "Epoch 49, loss: 630.158034\n",
      "Epoch 50, loss: 610.108433\n",
      "Epoch 51, loss: 629.078655\n",
      "Epoch 52, loss: 646.992136\n",
      "Epoch 53, loss: 632.109352\n",
      "Epoch 54, loss: 646.159428\n",
      "Epoch 55, loss: 633.179816\n",
      "Epoch 56, loss: 635.740073\n",
      "Epoch 57, loss: 644.576099\n",
      "Epoch 58, loss: 629.730003\n",
      "Epoch 59, loss: 623.307683\n",
      "Epoch 60, loss: 619.968828\n",
      "Epoch 61, loss: 653.477455\n",
      "Epoch 62, loss: 619.857795\n",
      "Epoch 63, loss: 621.724250\n",
      "Epoch 64, loss: 619.932812\n",
      "Epoch 65, loss: 637.331525\n",
      "Epoch 66, loss: 636.555539\n",
      "Epoch 67, loss: 623.057995\n",
      "Epoch 68, loss: 639.417316\n",
      "Epoch 69, loss: 648.223779\n",
      "Epoch 70, loss: 634.400789\n",
      "Epoch 71, loss: 618.635310\n",
      "Epoch 72, loss: 616.938480\n",
      "Epoch 73, loss: 619.551079\n",
      "Epoch 74, loss: 633.845678\n",
      "Epoch 75, loss: 635.295354\n",
      "Epoch 76, loss: 618.712638\n",
      "Epoch 77, loss: 633.957996\n",
      "Epoch 78, loss: 634.913853\n",
      "Epoch 79, loss: 634.303781\n",
      "Epoch 80, loss: 625.566030\n",
      "Epoch 81, loss: 624.206037\n",
      "Epoch 82, loss: 633.308530\n",
      "Epoch 83, loss: 628.866819\n",
      "Epoch 84, loss: 596.419326\n",
      "Epoch 85, loss: 631.133364\n",
      "Epoch 86, loss: 615.670764\n",
      "Epoch 87, loss: 629.445389\n",
      "Epoch 88, loss: 634.888095\n",
      "Epoch 89, loss: 613.523947\n",
      "Epoch 90, loss: 637.671830\n",
      "Epoch 91, loss: 638.969513\n",
      "Epoch 92, loss: 622.853672\n",
      "Epoch 93, loss: 625.726243\n",
      "Epoch 94, loss: 636.245330\n",
      "Epoch 95, loss: 628.041843\n",
      "Epoch 96, loss: 615.903021\n",
      "Epoch 97, loss: 629.469810\n",
      "Epoch 98, loss: 613.603680\n",
      "Epoch 99, loss: 628.309877\n",
      "Epoch 100, loss: 629.541219\n",
      "Epoch 101, loss: 633.196295\n",
      "Epoch 102, loss: 635.948032\n",
      "Epoch 103, loss: 623.344779\n",
      "Epoch 104, loss: 653.034018\n",
      "Epoch 105, loss: 623.578979\n",
      "Epoch 106, loss: 629.295813\n",
      "Epoch 107, loss: 622.928824\n",
      "Epoch 108, loss: 629.226352\n",
      "Epoch 109, loss: 648.307798\n",
      "Epoch 110, loss: 628.554753\n",
      "Epoch 111, loss: 613.202424\n",
      "Epoch 112, loss: 614.686600\n",
      "Epoch 113, loss: 620.694868\n",
      "Epoch 114, loss: 613.812858\n",
      "Epoch 115, loss: 624.646847\n",
      "Epoch 116, loss: 646.977025\n",
      "Epoch 117, loss: 617.259730\n",
      "Epoch 118, loss: 633.333557\n",
      "Epoch 119, loss: 630.506729\n",
      "Epoch 120, loss: 624.435322\n",
      "Epoch 121, loss: 618.495191\n",
      "Epoch 122, loss: 616.155119\n",
      "Epoch 123, loss: 621.148005\n",
      "Epoch 124, loss: 623.422526\n",
      "Epoch 125, loss: 622.425774\n",
      "Epoch 126, loss: 623.948120\n",
      "Epoch 127, loss: 611.712857\n",
      "Epoch 128, loss: 634.560965\n",
      "Epoch 129, loss: 594.778960\n",
      "Epoch 130, loss: 604.284474\n",
      "Epoch 131, loss: 628.595579\n",
      "Epoch 132, loss: 604.652863\n",
      "Epoch 133, loss: 627.149826\n",
      "Epoch 134, loss: 614.890135\n",
      "Epoch 135, loss: 633.916873\n",
      "Epoch 136, loss: 613.494027\n",
      "Epoch 137, loss: 632.513393\n",
      "Epoch 138, loss: 640.909395\n",
      "Epoch 139, loss: 622.318984\n",
      "Epoch 140, loss: 633.266542\n",
      "Epoch 141, loss: 638.353454\n",
      "Epoch 142, loss: 623.212043\n",
      "Epoch 143, loss: 613.688356\n",
      "Epoch 144, loss: 633.288444\n",
      "Epoch 145, loss: 607.757476\n",
      "Epoch 146, loss: 625.509838\n",
      "Epoch 147, loss: 617.351317\n",
      "Epoch 148, loss: 630.886542\n",
      "Epoch 149, loss: 599.456340\n",
      "Epoch 150, loss: 624.427960\n",
      "Epoch 151, loss: 631.303918\n",
      "Epoch 152, loss: 623.073120\n",
      "Epoch 153, loss: 608.380822\n",
      "Epoch 154, loss: 615.359153\n",
      "Epoch 155, loss: 635.418217\n",
      "Epoch 156, loss: 621.904909\n",
      "Epoch 157, loss: 599.402160\n",
      "Epoch 158, loss: 608.442324\n",
      "Epoch 159, loss: 617.839473\n",
      "Epoch 160, loss: 621.255131\n",
      "Epoch 161, loss: 611.973194\n",
      "Epoch 162, loss: 617.041196\n",
      "Epoch 163, loss: 629.740330\n",
      "Epoch 164, loss: 615.801429\n",
      "Epoch 165, loss: 616.931177\n",
      "Epoch 166, loss: 615.055188\n",
      "Epoch 167, loss: 640.895428\n",
      "Epoch 168, loss: 618.373903\n",
      "Epoch 169, loss: 638.786017\n",
      "Epoch 170, loss: 614.736029\n",
      "Epoch 171, loss: 643.791785\n",
      "Epoch 172, loss: 639.359907\n",
      "Epoch 173, loss: 620.288482\n",
      "Epoch 174, loss: 617.422681\n",
      "Epoch 175, loss: 614.216205\n",
      "Epoch 176, loss: 614.583829\n",
      "Epoch 177, loss: 634.462140\n",
      "Epoch 178, loss: 621.842583\n",
      "Epoch 179, loss: 605.729009\n",
      "Epoch 180, loss: 615.107781\n",
      "Epoch 181, loss: 620.087537\n",
      "Epoch 182, loss: 627.022491\n",
      "Epoch 183, loss: 604.093631\n",
      "Epoch 184, loss: 620.424637\n",
      "Epoch 185, loss: 617.296997\n",
      "Epoch 186, loss: 623.749339\n",
      "Epoch 187, loss: 607.791704\n",
      "Epoch 188, loss: 621.417918\n",
      "Epoch 189, loss: 591.812959\n",
      "Epoch 190, loss: 632.550575\n",
      "Epoch 191, loss: 620.932860\n",
      "Epoch 192, loss: 631.381799\n",
      "Epoch 193, loss: 608.921088\n",
      "Epoch 194, loss: 620.378560\n",
      "Epoch 195, loss: 612.755094\n",
      "Epoch 196, loss: 612.171757\n",
      "Epoch 197, loss: 616.974599\n",
      "Epoch 198, loss: 616.386263\n",
      "Epoch 199, loss: 624.827150\n",
      "Validation accuracy:  0.244 lr:  0.0001 reg:  1e-05\n",
      "Epoch 0, loss: 685.449305\n",
      "Epoch 1, loss: 678.438002\n",
      "Epoch 2, loss: 675.375091\n",
      "Epoch 3, loss: 674.680924\n",
      "Epoch 4, loss: 671.042896\n",
      "Epoch 5, loss: 659.332515\n",
      "Epoch 6, loss: 660.476584\n",
      "Epoch 7, loss: 660.002899\n",
      "Epoch 8, loss: 654.268915\n",
      "Epoch 9, loss: 660.139615\n",
      "Epoch 10, loss: 658.697913\n",
      "Epoch 11, loss: 663.881431\n",
      "Epoch 12, loss: 655.447319\n",
      "Epoch 13, loss: 656.287169\n",
      "Epoch 14, loss: 635.539173\n",
      "Epoch 15, loss: 654.906955\n",
      "Epoch 16, loss: 653.248221\n",
      "Epoch 17, loss: 658.515053\n",
      "Epoch 18, loss: 653.231643\n",
      "Epoch 19, loss: 641.826309\n",
      "Epoch 20, loss: 652.708516\n",
      "Epoch 21, loss: 634.614545\n",
      "Epoch 22, loss: 656.571519\n",
      "Epoch 23, loss: 632.355973\n",
      "Epoch 24, loss: 647.595797\n",
      "Epoch 25, loss: 645.927924\n",
      "Epoch 26, loss: 635.338715\n",
      "Epoch 27, loss: 640.954208\n",
      "Epoch 28, loss: 623.933778\n",
      "Epoch 29, loss: 638.671792\n",
      "Epoch 30, loss: 645.067309\n",
      "Epoch 31, loss: 628.865656\n",
      "Epoch 32, loss: 644.424326\n",
      "Epoch 33, loss: 640.300903\n",
      "Epoch 34, loss: 632.104771\n",
      "Epoch 35, loss: 637.499748\n",
      "Epoch 36, loss: 639.577908\n",
      "Epoch 37, loss: 639.966898\n",
      "Epoch 38, loss: 636.382143\n",
      "Epoch 39, loss: 626.395542\n",
      "Epoch 40, loss: 641.777270\n",
      "Epoch 41, loss: 640.005931\n",
      "Epoch 42, loss: 642.261716\n",
      "Epoch 43, loss: 634.320057\n",
      "Epoch 44, loss: 640.652358\n",
      "Epoch 45, loss: 640.356648\n",
      "Epoch 46, loss: 624.333293\n",
      "Epoch 47, loss: 635.970406\n",
      "Epoch 48, loss: 637.539542\n",
      "Epoch 49, loss: 651.090887\n",
      "Epoch 50, loss: 634.613993\n",
      "Epoch 51, loss: 626.152332\n",
      "Epoch 52, loss: 641.863231\n",
      "Epoch 53, loss: 622.643881\n",
      "Epoch 54, loss: 609.352706\n",
      "Epoch 55, loss: 628.066908\n",
      "Epoch 56, loss: 636.335896\n",
      "Epoch 57, loss: 636.963306\n",
      "Epoch 58, loss: 619.233907\n",
      "Epoch 59, loss: 629.703401\n",
      "Epoch 60, loss: 635.048846\n",
      "Epoch 61, loss: 626.214603\n",
      "Epoch 62, loss: 626.250920\n",
      "Epoch 63, loss: 623.329542\n",
      "Epoch 64, loss: 627.884503\n",
      "Epoch 65, loss: 623.650609\n",
      "Epoch 66, loss: 637.270609\n",
      "Epoch 67, loss: 614.501772\n",
      "Epoch 68, loss: 635.753412\n",
      "Epoch 69, loss: 623.867812\n",
      "Epoch 70, loss: 642.797118\n",
      "Epoch 71, loss: 628.850687\n",
      "Epoch 72, loss: 635.460491\n",
      "Epoch 73, loss: 625.222916\n",
      "Epoch 74, loss: 620.686531\n",
      "Epoch 75, loss: 624.549418\n",
      "Epoch 76, loss: 634.019526\n",
      "Epoch 77, loss: 627.500543\n",
      "Epoch 78, loss: 632.617871\n",
      "Epoch 79, loss: 639.396664\n",
      "Epoch 80, loss: 641.209742\n",
      "Epoch 81, loss: 617.042520\n",
      "Epoch 82, loss: 647.200221\n",
      "Epoch 83, loss: 645.029362\n",
      "Epoch 84, loss: 620.491415\n",
      "Epoch 85, loss: 622.501232\n",
      "Epoch 86, loss: 621.093296\n",
      "Epoch 87, loss: 627.666912\n",
      "Epoch 88, loss: 630.632028\n",
      "Epoch 89, loss: 629.566084\n",
      "Epoch 90, loss: 615.436093\n",
      "Epoch 91, loss: 615.045781\n",
      "Epoch 92, loss: 664.079133\n",
      "Epoch 93, loss: 641.870480\n",
      "Epoch 94, loss: 621.566512\n",
      "Epoch 95, loss: 634.958386\n",
      "Epoch 96, loss: 615.995669\n",
      "Epoch 97, loss: 632.310603\n",
      "Epoch 98, loss: 628.629040\n",
      "Epoch 99, loss: 630.879206\n",
      "Epoch 100, loss: 610.569988\n",
      "Epoch 101, loss: 635.395034\n",
      "Epoch 102, loss: 638.639827\n",
      "Epoch 103, loss: 649.625540\n",
      "Epoch 104, loss: 620.761373\n",
      "Epoch 105, loss: 611.719664\n",
      "Epoch 106, loss: 630.238656\n",
      "Epoch 107, loss: 615.233965\n",
      "Epoch 108, loss: 636.608679\n",
      "Epoch 109, loss: 624.288680\n",
      "Epoch 110, loss: 642.315853\n",
      "Epoch 111, loss: 628.357940\n",
      "Epoch 112, loss: 632.967625\n",
      "Epoch 113, loss: 607.139563\n",
      "Epoch 114, loss: 640.474314\n",
      "Epoch 115, loss: 629.083217\n",
      "Epoch 116, loss: 646.371954\n",
      "Epoch 117, loss: 626.968762\n",
      "Epoch 118, loss: 615.857319\n",
      "Epoch 119, loss: 613.756372\n",
      "Epoch 120, loss: 612.912805\n",
      "Epoch 121, loss: 636.915461\n",
      "Epoch 122, loss: 634.964252\n",
      "Epoch 123, loss: 614.703627\n",
      "Epoch 124, loss: 614.691997\n",
      "Epoch 125, loss: 628.709002\n",
      "Epoch 126, loss: 621.631864\n",
      "Epoch 127, loss: 627.053498\n",
      "Epoch 128, loss: 610.361498\n",
      "Epoch 129, loss: 620.982607\n",
      "Epoch 130, loss: 629.891024\n",
      "Epoch 131, loss: 629.833937\n",
      "Epoch 132, loss: 629.207591\n",
      "Epoch 133, loss: 608.667305\n",
      "Epoch 134, loss: 617.961973\n",
      "Epoch 135, loss: 617.201365\n",
      "Epoch 136, loss: 633.209526\n",
      "Epoch 137, loss: 626.583983\n",
      "Epoch 138, loss: 623.299643\n",
      "Epoch 139, loss: 628.953807\n",
      "Epoch 140, loss: 609.730707\n",
      "Epoch 141, loss: 620.831864\n",
      "Epoch 142, loss: 631.159769\n",
      "Epoch 143, loss: 628.546103\n",
      "Epoch 144, loss: 628.448756\n",
      "Epoch 145, loss: 618.089199\n",
      "Epoch 146, loss: 602.743709\n",
      "Epoch 147, loss: 643.691408\n",
      "Epoch 148, loss: 614.748388\n",
      "Epoch 149, loss: 613.642743\n",
      "Epoch 150, loss: 623.881044\n",
      "Epoch 151, loss: 634.734783\n",
      "Epoch 152, loss: 614.446483\n",
      "Epoch 153, loss: 606.185504\n",
      "Epoch 154, loss: 630.443332\n",
      "Epoch 155, loss: 612.482305\n",
      "Epoch 156, loss: 622.856948\n",
      "Epoch 157, loss: 659.111033\n",
      "Epoch 158, loss: 617.636804\n",
      "Epoch 159, loss: 617.478777\n",
      "Epoch 160, loss: 611.916731\n",
      "Epoch 161, loss: 627.061829\n",
      "Epoch 162, loss: 613.469141\n",
      "Epoch 163, loss: 632.719978\n",
      "Epoch 164, loss: 604.921632\n",
      "Epoch 165, loss: 603.190104\n",
      "Epoch 166, loss: 612.172778\n",
      "Epoch 167, loss: 637.339451\n",
      "Epoch 168, loss: 633.484062\n",
      "Epoch 169, loss: 622.528151\n",
      "Epoch 170, loss: 619.981388\n",
      "Epoch 171, loss: 628.153321\n",
      "Epoch 172, loss: 623.664839\n",
      "Epoch 173, loss: 621.075572\n",
      "Epoch 174, loss: 595.186153\n",
      "Epoch 175, loss: 621.187362\n",
      "Epoch 176, loss: 615.498636\n",
      "Epoch 177, loss: 612.080020\n",
      "Epoch 178, loss: 620.181772\n",
      "Epoch 179, loss: 606.319205\n",
      "Epoch 180, loss: 610.668685\n",
      "Epoch 181, loss: 620.225497\n",
      "Epoch 182, loss: 619.514796\n",
      "Epoch 183, loss: 633.508282\n",
      "Epoch 184, loss: 618.194062\n",
      "Epoch 185, loss: 611.591331\n",
      "Epoch 186, loss: 614.725158\n",
      "Epoch 187, loss: 614.846829\n",
      "Epoch 188, loss: 596.525424\n",
      "Epoch 189, loss: 616.489464\n",
      "Epoch 190, loss: 620.820423\n",
      "Epoch 191, loss: 638.712654\n",
      "Epoch 192, loss: 617.329060\n",
      "Epoch 193, loss: 632.232707\n",
      "Epoch 194, loss: 609.056376\n",
      "Epoch 195, loss: 612.608083\n",
      "Epoch 196, loss: 614.357523\n",
      "Epoch 197, loss: 606.712586\n",
      "Epoch 198, loss: 596.861826\n",
      "Epoch 199, loss: 601.054624\n",
      "Validation accuracy:  0.246 lr:  0.0001 reg:  0.0001\n",
      "Epoch 0, loss: 688.473262\n",
      "Epoch 1, loss: 681.352618\n",
      "Epoch 2, loss: 673.678366\n",
      "Epoch 3, loss: 667.695371\n",
      "Epoch 4, loss: 665.510624\n",
      "Epoch 5, loss: 660.811756\n",
      "Epoch 6, loss: 665.669698\n",
      "Epoch 7, loss: 649.833286\n",
      "Epoch 8, loss: 656.376266\n",
      "Epoch 9, loss: 651.819216\n",
      "Epoch 10, loss: 653.461618\n",
      "Epoch 11, loss: 652.799513\n",
      "Epoch 12, loss: 658.638608\n",
      "Epoch 13, loss: 656.107375\n",
      "Epoch 14, loss: 650.094653\n",
      "Epoch 15, loss: 647.930096\n",
      "Epoch 16, loss: 645.670011\n",
      "Epoch 17, loss: 644.643371\n",
      "Epoch 18, loss: 640.481411\n",
      "Epoch 19, loss: 660.930030\n",
      "Epoch 20, loss: 649.234665\n",
      "Epoch 21, loss: 648.428514\n",
      "Epoch 22, loss: 642.814893\n",
      "Epoch 23, loss: 643.415954\n",
      "Epoch 24, loss: 643.425236\n",
      "Epoch 25, loss: 644.716926\n",
      "Epoch 26, loss: 624.965193\n",
      "Epoch 27, loss: 640.752441\n",
      "Epoch 28, loss: 634.382802\n",
      "Epoch 29, loss: 641.996718\n",
      "Epoch 30, loss: 616.473814\n",
      "Epoch 31, loss: 637.599296\n",
      "Epoch 32, loss: 635.678722\n",
      "Epoch 33, loss: 641.332131\n",
      "Epoch 34, loss: 629.153174\n",
      "Epoch 35, loss: 639.714124\n",
      "Epoch 36, loss: 634.611887\n",
      "Epoch 37, loss: 636.340477\n",
      "Epoch 38, loss: 629.256881\n",
      "Epoch 39, loss: 637.965381\n",
      "Epoch 40, loss: 626.766289\n",
      "Epoch 41, loss: 654.882697\n",
      "Epoch 42, loss: 631.379710\n",
      "Epoch 43, loss: 633.511489\n",
      "Epoch 44, loss: 630.458592\n",
      "Epoch 45, loss: 633.241904\n",
      "Epoch 46, loss: 648.678458\n",
      "Epoch 47, loss: 618.775834\n",
      "Epoch 48, loss: 645.281680\n",
      "Epoch 49, loss: 630.249196\n",
      "Epoch 50, loss: 634.609539\n",
      "Epoch 51, loss: 632.209302\n",
      "Epoch 52, loss: 646.817116\n",
      "Epoch 53, loss: 636.182386\n",
      "Epoch 54, loss: 640.758260\n",
      "Epoch 55, loss: 621.030219\n",
      "Epoch 56, loss: 622.804662\n",
      "Epoch 57, loss: 649.107893\n",
      "Epoch 58, loss: 626.970989\n",
      "Epoch 59, loss: 658.550544\n",
      "Epoch 60, loss: 628.158986\n",
      "Epoch 61, loss: 639.200023\n",
      "Epoch 62, loss: 619.292843\n",
      "Epoch 63, loss: 624.040628\n",
      "Epoch 64, loss: 635.340234\n",
      "Epoch 65, loss: 636.642854\n",
      "Epoch 66, loss: 619.342257\n",
      "Epoch 67, loss: 628.799700\n",
      "Epoch 68, loss: 635.121792\n",
      "Epoch 69, loss: 643.517704\n",
      "Epoch 70, loss: 652.142992\n",
      "Epoch 71, loss: 621.333306\n",
      "Epoch 72, loss: 651.825506\n",
      "Epoch 73, loss: 622.239052\n",
      "Epoch 74, loss: 658.100212\n",
      "Epoch 75, loss: 632.481681\n",
      "Epoch 76, loss: 641.762946\n",
      "Epoch 77, loss: 609.754743\n",
      "Epoch 78, loss: 635.503443\n",
      "Epoch 79, loss: 632.738265\n",
      "Epoch 80, loss: 617.996673\n",
      "Epoch 81, loss: 623.702846\n",
      "Epoch 82, loss: 639.919878\n",
      "Epoch 83, loss: 626.356665\n",
      "Epoch 84, loss: 619.476244\n",
      "Epoch 85, loss: 629.492296\n",
      "Epoch 86, loss: 623.665716\n",
      "Epoch 87, loss: 634.713139\n",
      "Epoch 88, loss: 638.229070\n",
      "Epoch 89, loss: 628.067650\n",
      "Epoch 90, loss: 622.976271\n",
      "Epoch 91, loss: 620.402253\n",
      "Epoch 92, loss: 637.370059\n",
      "Epoch 93, loss: 612.819716\n",
      "Epoch 94, loss: 638.148043\n",
      "Epoch 95, loss: 639.840965\n",
      "Epoch 96, loss: 624.047574\n",
      "Epoch 97, loss: 623.964755\n",
      "Epoch 98, loss: 643.817362\n",
      "Epoch 99, loss: 607.985200\n",
      "Epoch 100, loss: 617.131676\n",
      "Epoch 101, loss: 612.119533\n",
      "Epoch 102, loss: 630.086199\n",
      "Epoch 103, loss: 640.319196\n",
      "Epoch 104, loss: 630.460789\n",
      "Epoch 105, loss: 636.483567\n",
      "Epoch 106, loss: 623.125451\n",
      "Epoch 107, loss: 612.991805\n",
      "Epoch 108, loss: 600.692755\n",
      "Epoch 109, loss: 614.201820\n",
      "Epoch 110, loss: 622.776486\n",
      "Epoch 111, loss: 644.419375\n",
      "Epoch 112, loss: 613.588406\n",
      "Epoch 113, loss: 609.274376\n",
      "Epoch 114, loss: 602.266481\n",
      "Epoch 115, loss: 656.340784\n",
      "Epoch 116, loss: 626.050105\n",
      "Epoch 117, loss: 618.860278\n",
      "Epoch 118, loss: 642.556528\n",
      "Epoch 119, loss: 631.627135\n",
      "Epoch 120, loss: 620.963696\n",
      "Epoch 121, loss: 638.218978\n",
      "Epoch 122, loss: 611.530317\n",
      "Epoch 123, loss: 623.896153\n",
      "Epoch 124, loss: 627.655900\n",
      "Epoch 125, loss: 632.085400\n",
      "Epoch 126, loss: 612.249132\n",
      "Epoch 127, loss: 640.617534\n",
      "Epoch 128, loss: 629.478288\n",
      "Epoch 129, loss: 621.406535\n",
      "Epoch 130, loss: 620.176429\n",
      "Epoch 131, loss: 606.718850\n",
      "Epoch 132, loss: 623.326822\n",
      "Epoch 133, loss: 621.783096\n",
      "Epoch 134, loss: 640.288720\n",
      "Epoch 135, loss: 608.522096\n",
      "Epoch 136, loss: 638.351495\n",
      "Epoch 137, loss: 614.948495\n",
      "Epoch 138, loss: 635.078527\n",
      "Epoch 139, loss: 618.522290\n",
      "Epoch 140, loss: 622.181834\n",
      "Epoch 141, loss: 602.465594\n",
      "Epoch 142, loss: 626.612895\n",
      "Epoch 143, loss: 621.881712\n",
      "Epoch 144, loss: 628.419928\n",
      "Epoch 145, loss: 616.200514\n",
      "Epoch 146, loss: 624.673804\n",
      "Epoch 147, loss: 637.669843\n",
      "Epoch 148, loss: 612.355193\n",
      "Epoch 149, loss: 601.774227\n",
      "Epoch 150, loss: 602.193609\n",
      "Epoch 151, loss: 627.273216\n",
      "Epoch 152, loss: 630.845346\n",
      "Epoch 153, loss: 621.481445\n",
      "Epoch 154, loss: 628.157649\n",
      "Epoch 155, loss: 629.673158\n",
      "Epoch 156, loss: 611.894830\n",
      "Epoch 157, loss: 611.189531\n",
      "Epoch 158, loss: 628.656770\n",
      "Epoch 159, loss: 633.758038\n",
      "Epoch 160, loss: 637.270337\n",
      "Epoch 161, loss: 620.993485\n",
      "Epoch 162, loss: 615.600095\n",
      "Epoch 163, loss: 623.232727\n",
      "Epoch 164, loss: 640.172534\n",
      "Epoch 165, loss: 634.188788\n",
      "Epoch 166, loss: 617.269683\n",
      "Epoch 167, loss: 627.397010\n",
      "Epoch 168, loss: 600.826001\n",
      "Epoch 169, loss: 610.042130\n",
      "Epoch 170, loss: 625.910286\n",
      "Epoch 171, loss: 623.626252\n",
      "Epoch 172, loss: 626.888794\n",
      "Epoch 173, loss: 633.185112\n",
      "Epoch 174, loss: 615.073094\n",
      "Epoch 175, loss: 632.140749\n",
      "Epoch 176, loss: 608.864089\n",
      "Epoch 177, loss: 600.629000\n",
      "Epoch 178, loss: 623.191611\n",
      "Epoch 179, loss: 611.649477\n",
      "Epoch 180, loss: 622.178092\n",
      "Epoch 181, loss: 611.868847\n",
      "Epoch 182, loss: 615.490705\n",
      "Epoch 183, loss: 597.689338\n",
      "Epoch 184, loss: 619.210372\n",
      "Epoch 185, loss: 619.469447\n",
      "Epoch 186, loss: 619.928869\n",
      "Epoch 187, loss: 631.812633\n",
      "Epoch 188, loss: 623.320278\n",
      "Epoch 189, loss: 624.076724\n",
      "Epoch 190, loss: 598.213875\n",
      "Epoch 191, loss: 633.551910\n",
      "Epoch 192, loss: 621.653710\n",
      "Epoch 193, loss: 637.369049\n",
      "Epoch 194, loss: 602.533865\n",
      "Epoch 195, loss: 620.885009\n",
      "Epoch 196, loss: 620.376274\n",
      "Epoch 197, loss: 617.393854\n",
      "Epoch 198, loss: 617.703526\n",
      "Epoch 199, loss: 627.682935\n",
      "Validation accuracy:  0.249 lr:  0.0001 reg:  0.001\n",
      "Epoch 0, loss: 877.478507\n",
      "Epoch 1, loss: 861.031017\n",
      "Epoch 2, loss: 905.703401\n",
      "Epoch 3, loss: 1039.029661\n",
      "Epoch 4, loss: 941.235143\n",
      "Epoch 5, loss: 973.552169\n",
      "Epoch 6, loss: 806.453219\n",
      "Epoch 7, loss: 821.716012\n",
      "Epoch 8, loss: 792.260294\n",
      "Epoch 9, loss: 908.125920\n",
      "Epoch 10, loss: 859.242534\n",
      "Epoch 11, loss: 829.746011\n",
      "Epoch 12, loss: 933.877497\n",
      "Epoch 13, loss: 700.719057\n",
      "Epoch 14, loss: 816.081316\n",
      "Epoch 15, loss: 808.409975\n",
      "Epoch 16, loss: 855.084950\n",
      "Epoch 17, loss: 790.868281\n",
      "Epoch 18, loss: 722.538631\n",
      "Epoch 19, loss: 810.492945\n",
      "Epoch 20, loss: 726.193263\n",
      "Epoch 21, loss: 855.255868\n",
      "Epoch 22, loss: 1048.856353\n",
      "Epoch 23, loss: 741.034226\n",
      "Epoch 24, loss: 747.328905\n",
      "Epoch 25, loss: 988.598637\n",
      "Epoch 26, loss: 754.128734\n",
      "Epoch 27, loss: 909.464654\n",
      "Epoch 28, loss: 826.354967\n",
      "Epoch 29, loss: 804.069873\n",
      "Epoch 30, loss: 797.863086\n",
      "Epoch 31, loss: 793.590459\n",
      "Epoch 32, loss: 726.462219\n",
      "Epoch 33, loss: 899.254963\n",
      "Epoch 34, loss: 701.563879\n",
      "Epoch 35, loss: 824.349337\n",
      "Epoch 36, loss: 792.971446\n",
      "Epoch 37, loss: 728.004223\n",
      "Epoch 38, loss: 677.662554\n",
      "Epoch 39, loss: 793.377829\n",
      "Epoch 40, loss: 706.604103\n",
      "Epoch 41, loss: 708.624250\n",
      "Epoch 42, loss: 741.699564\n",
      "Epoch 43, loss: 706.235223\n",
      "Epoch 44, loss: 903.404752\n",
      "Epoch 45, loss: 794.330808\n",
      "Epoch 46, loss: 643.391461\n",
      "Epoch 47, loss: 685.841416\n",
      "Epoch 48, loss: 796.420740\n",
      "Epoch 49, loss: 758.049285\n",
      "Epoch 50, loss: 869.523329\n",
      "Epoch 51, loss: 807.772739\n",
      "Epoch 52, loss: 754.874203\n",
      "Epoch 53, loss: 856.845926\n",
      "Epoch 54, loss: 725.634288\n",
      "Epoch 55, loss: 684.850663\n",
      "Epoch 56, loss: 683.019090\n",
      "Epoch 57, loss: 740.382602\n",
      "Epoch 58, loss: 853.092814\n",
      "Epoch 59, loss: 665.731254\n",
      "Epoch 60, loss: 904.673990\n",
      "Epoch 61, loss: 712.241532\n",
      "Epoch 62, loss: 677.082026\n",
      "Epoch 63, loss: 700.756770\n",
      "Epoch 64, loss: 870.956647\n",
      "Epoch 65, loss: 676.894899\n",
      "Epoch 66, loss: 697.010662\n",
      "Epoch 67, loss: 740.217213\n",
      "Epoch 68, loss: 739.954295\n",
      "Epoch 69, loss: 720.985456\n",
      "Epoch 70, loss: 810.600071\n",
      "Epoch 71, loss: 760.652053\n",
      "Epoch 72, loss: 790.008705\n",
      "Epoch 73, loss: 643.197136\n",
      "Epoch 74, loss: 742.503427\n",
      "Epoch 75, loss: 642.208957\n",
      "Epoch 76, loss: 906.694039\n",
      "Epoch 77, loss: 863.308111\n",
      "Epoch 78, loss: 771.767483\n",
      "Epoch 79, loss: 872.096681\n",
      "Epoch 80, loss: 794.456549\n",
      "Epoch 81, loss: 754.837647\n",
      "Epoch 82, loss: 774.031057\n",
      "Epoch 83, loss: 794.866979\n",
      "Epoch 84, loss: 763.132527\n",
      "Epoch 85, loss: 659.914185\n",
      "Epoch 86, loss: 781.122974\n",
      "Epoch 87, loss: 723.695596\n",
      "Epoch 88, loss: 898.416786\n",
      "Epoch 89, loss: 818.104011\n",
      "Epoch 90, loss: 661.045263\n",
      "Epoch 91, loss: 742.881730\n",
      "Epoch 92, loss: 740.324927\n",
      "Epoch 93, loss: 848.317506\n",
      "Epoch 94, loss: 818.984547\n",
      "Epoch 95, loss: 731.881275\n",
      "Epoch 96, loss: 715.830354\n",
      "Epoch 97, loss: 746.621925\n",
      "Epoch 98, loss: 607.541720\n",
      "Epoch 99, loss: 788.757141\n",
      "Epoch 100, loss: 789.880239\n",
      "Epoch 101, loss: 807.445794\n",
      "Epoch 102, loss: 808.255745\n",
      "Epoch 103, loss: 665.658317\n",
      "Epoch 104, loss: 721.568072\n",
      "Epoch 105, loss: 946.269244\n",
      "Epoch 106, loss: 897.474749\n",
      "Epoch 107, loss: 753.993997\n",
      "Epoch 108, loss: 738.708620\n",
      "Epoch 109, loss: 828.341484\n",
      "Epoch 110, loss: 687.843872\n",
      "Epoch 111, loss: 721.615870\n",
      "Epoch 112, loss: 809.135750\n",
      "Epoch 113, loss: 759.950047\n",
      "Epoch 114, loss: 901.519694\n",
      "Epoch 115, loss: 675.482621\n",
      "Epoch 116, loss: 769.600042\n",
      "Epoch 117, loss: 898.542644\n",
      "Epoch 118, loss: 691.757745\n",
      "Epoch 119, loss: 759.777287\n",
      "Epoch 120, loss: 799.117048\n",
      "Epoch 121, loss: 752.199475\n",
      "Epoch 122, loss: 762.068786\n",
      "Epoch 123, loss: 678.411648\n",
      "Epoch 124, loss: 698.996743\n",
      "Epoch 125, loss: 797.270030\n",
      "Epoch 126, loss: 953.326998\n",
      "Epoch 127, loss: 668.709359\n",
      "Epoch 128, loss: 843.374343\n",
      "Epoch 129, loss: 757.913620\n",
      "Epoch 130, loss: 703.551260\n",
      "Epoch 131, loss: 637.420834\n",
      "Epoch 132, loss: 912.359274\n",
      "Epoch 133, loss: 710.065889\n",
      "Epoch 134, loss: 856.913894\n",
      "Epoch 135, loss: 727.025592\n",
      "Epoch 136, loss: 827.829119\n",
      "Epoch 137, loss: 710.260942\n",
      "Epoch 138, loss: 659.148533\n",
      "Epoch 139, loss: 738.632794\n",
      "Epoch 140, loss: 858.983210\n",
      "Epoch 141, loss: 994.256153\n",
      "Epoch 142, loss: 661.898739\n",
      "Epoch 143, loss: 704.222131\n",
      "Epoch 144, loss: 856.562104\n",
      "Epoch 145, loss: 849.674027\n",
      "Epoch 146, loss: 730.223237\n",
      "Epoch 147, loss: 760.466947\n",
      "Epoch 148, loss: 790.209508\n",
      "Epoch 149, loss: 663.076447\n",
      "Epoch 150, loss: 961.148622\n",
      "Epoch 151, loss: 788.111225\n",
      "Epoch 152, loss: 756.549903\n",
      "Epoch 153, loss: 702.766414\n",
      "Epoch 154, loss: 868.634990\n",
      "Epoch 155, loss: 711.700587\n",
      "Epoch 156, loss: 758.251767\n",
      "Epoch 157, loss: 827.261996\n",
      "Epoch 158, loss: 824.767641\n",
      "Epoch 159, loss: 788.015002\n",
      "Epoch 160, loss: 983.102360\n",
      "Epoch 161, loss: 790.630245\n",
      "Epoch 162, loss: 774.552507\n",
      "Epoch 163, loss: 893.377736\n",
      "Epoch 164, loss: 855.532927\n",
      "Epoch 165, loss: 795.815307\n",
      "Epoch 166, loss: 695.964107\n",
      "Epoch 167, loss: 685.920390\n",
      "Epoch 168, loss: 613.034360\n",
      "Epoch 169, loss: 852.807368\n",
      "Epoch 170, loss: 712.129988\n",
      "Epoch 171, loss: 850.768964\n",
      "Epoch 172, loss: 731.954362\n",
      "Epoch 173, loss: 691.109235\n",
      "Epoch 174, loss: 699.260739\n",
      "Epoch 175, loss: 771.011610\n",
      "Epoch 176, loss: 644.690031\n",
      "Epoch 177, loss: 765.742012\n",
      "Epoch 178, loss: 668.054947\n",
      "Epoch 179, loss: 683.848896\n",
      "Epoch 180, loss: 707.151777\n",
      "Epoch 181, loss: 720.248152\n",
      "Epoch 182, loss: 737.328788\n",
      "Epoch 183, loss: 949.607666\n",
      "Epoch 184, loss: 771.487550\n",
      "Epoch 185, loss: 744.310263\n",
      "Epoch 186, loss: 764.317308\n",
      "Epoch 187, loss: 855.524458\n",
      "Epoch 188, loss: 760.189235\n",
      "Epoch 189, loss: 989.341090\n",
      "Epoch 190, loss: 674.261299\n",
      "Epoch 191, loss: 784.927498\n",
      "Epoch 192, loss: 624.796131\n",
      "Epoch 193, loss: 684.677609\n",
      "Epoch 194, loss: 862.881393\n",
      "Epoch 195, loss: 858.617493\n",
      "Epoch 196, loss: 814.843973\n",
      "Epoch 197, loss: 938.320714\n",
      "Epoch 198, loss: 795.367221\n",
      "Epoch 199, loss: 607.162213\n",
      "Validation accuracy:  0.212 lr:  0.001 reg:  1e-06\n",
      "Epoch 0, loss: 876.678940\n",
      "Epoch 1, loss: 836.257462\n",
      "Epoch 2, loss: 877.117089\n",
      "Epoch 3, loss: 784.557735\n",
      "Epoch 4, loss: 901.128246\n",
      "Epoch 5, loss: 952.019402\n",
      "Epoch 6, loss: 670.773146\n",
      "Epoch 7, loss: 871.876241\n",
      "Epoch 8, loss: 815.741676\n",
      "Epoch 9, loss: 740.403265\n",
      "Epoch 10, loss: 725.182670\n",
      "Epoch 11, loss: 810.210532\n",
      "Epoch 12, loss: 902.246491\n",
      "Epoch 13, loss: 728.941550\n",
      "Epoch 14, loss: 830.705124\n",
      "Epoch 15, loss: 846.718020\n",
      "Epoch 16, loss: 885.512260\n",
      "Epoch 17, loss: 860.610955\n",
      "Epoch 18, loss: 741.081869\n",
      "Epoch 19, loss: 781.963139\n",
      "Epoch 20, loss: 819.244023\n",
      "Epoch 21, loss: 839.257768\n",
      "Epoch 22, loss: 692.842377\n",
      "Epoch 23, loss: 959.524598\n",
      "Epoch 24, loss: 809.962808\n",
      "Epoch 25, loss: 699.284404\n",
      "Epoch 26, loss: 884.200688\n",
      "Epoch 27, loss: 784.416749\n",
      "Epoch 28, loss: 823.994197\n",
      "Epoch 29, loss: 815.608071\n",
      "Epoch 30, loss: 856.042044\n",
      "Epoch 31, loss: 801.571547\n",
      "Epoch 32, loss: 865.363079\n",
      "Epoch 33, loss: 769.317418\n",
      "Epoch 34, loss: 814.128804\n",
      "Epoch 35, loss: 853.607093\n",
      "Epoch 36, loss: 753.229727\n",
      "Epoch 37, loss: 795.874949\n",
      "Epoch 38, loss: 837.473064\n",
      "Epoch 39, loss: 763.347301\n",
      "Epoch 40, loss: 826.441710\n",
      "Epoch 41, loss: 734.697836\n",
      "Epoch 42, loss: 675.597063\n",
      "Epoch 43, loss: 807.211112\n",
      "Epoch 44, loss: 673.756153\n",
      "Epoch 45, loss: 764.357651\n",
      "Epoch 46, loss: 670.511346\n",
      "Epoch 47, loss: 662.411502\n",
      "Epoch 48, loss: 1023.942225\n",
      "Epoch 49, loss: 873.103370\n",
      "Epoch 50, loss: 766.712854\n",
      "Epoch 51, loss: 674.364584\n",
      "Epoch 52, loss: 816.960535\n",
      "Epoch 53, loss: 763.763264\n",
      "Epoch 54, loss: 734.917279\n",
      "Epoch 55, loss: 858.428483\n",
      "Epoch 56, loss: 719.521479\n",
      "Epoch 57, loss: 750.965574\n",
      "Epoch 58, loss: 688.774320\n",
      "Epoch 59, loss: 911.128625\n",
      "Epoch 60, loss: 801.143425\n",
      "Epoch 61, loss: 824.749421\n",
      "Epoch 62, loss: 955.264952\n",
      "Epoch 63, loss: 822.075879\n",
      "Epoch 64, loss: 634.140377\n",
      "Epoch 65, loss: 710.282889\n",
      "Epoch 66, loss: 732.622483\n",
      "Epoch 67, loss: 806.156320\n",
      "Epoch 68, loss: 804.139331\n",
      "Epoch 69, loss: 954.000831\n",
      "Epoch 70, loss: 769.080660\n",
      "Epoch 71, loss: 770.557503\n",
      "Epoch 72, loss: 720.201551\n",
      "Epoch 73, loss: 734.576049\n",
      "Epoch 74, loss: 709.244965\n",
      "Epoch 75, loss: 675.146969\n",
      "Epoch 76, loss: 907.612061\n",
      "Epoch 77, loss: 783.733276\n",
      "Epoch 78, loss: 708.044705\n",
      "Epoch 79, loss: 660.707054\n",
      "Epoch 80, loss: 801.314077\n",
      "Epoch 81, loss: 776.375675\n",
      "Epoch 82, loss: 736.212631\n",
      "Epoch 83, loss: 899.185648\n",
      "Epoch 84, loss: 823.442063\n",
      "Epoch 85, loss: 695.866808\n",
      "Epoch 86, loss: 763.044294\n",
      "Epoch 87, loss: 837.402904\n",
      "Epoch 88, loss: 938.274446\n",
      "Epoch 89, loss: 810.765701\n",
      "Epoch 90, loss: 758.237869\n",
      "Epoch 91, loss: 713.632029\n",
      "Epoch 92, loss: 685.716122\n",
      "Epoch 93, loss: 792.973469\n",
      "Epoch 94, loss: 919.661983\n",
      "Epoch 95, loss: 869.224186\n",
      "Epoch 96, loss: 746.035952\n",
      "Epoch 97, loss: 815.744213\n",
      "Epoch 98, loss: 672.351940\n",
      "Epoch 99, loss: 807.833294\n",
      "Epoch 100, loss: 854.391882\n",
      "Epoch 101, loss: 739.453652\n",
      "Epoch 102, loss: 664.772431\n",
      "Epoch 103, loss: 809.496688\n",
      "Epoch 104, loss: 659.609911\n",
      "Epoch 105, loss: 753.855929\n",
      "Epoch 106, loss: 694.809748\n",
      "Epoch 107, loss: 955.371267\n",
      "Epoch 108, loss: 825.212245\n",
      "Epoch 109, loss: 630.604687\n",
      "Epoch 110, loss: 729.595437\n",
      "Epoch 111, loss: 665.221139\n",
      "Epoch 112, loss: 802.621483\n",
      "Epoch 113, loss: 787.037660\n",
      "Epoch 114, loss: 830.026523\n",
      "Epoch 115, loss: 781.244876\n",
      "Epoch 116, loss: 824.327753\n",
      "Epoch 117, loss: 748.582859\n",
      "Epoch 118, loss: 749.401396\n",
      "Epoch 119, loss: 649.274434\n",
      "Epoch 120, loss: 730.532284\n",
      "Epoch 121, loss: 849.373952\n",
      "Epoch 122, loss: 659.584459\n",
      "Epoch 123, loss: 805.940278\n",
      "Epoch 124, loss: 769.546607\n",
      "Epoch 125, loss: 913.124788\n",
      "Epoch 126, loss: 832.443338\n",
      "Epoch 127, loss: 899.454547\n",
      "Epoch 128, loss: 804.093288\n",
      "Epoch 129, loss: 768.443790\n",
      "Epoch 130, loss: 765.080464\n",
      "Epoch 131, loss: 841.839655\n",
      "Epoch 132, loss: 622.675932\n",
      "Epoch 133, loss: 983.837539\n",
      "Epoch 134, loss: 800.727039\n",
      "Epoch 135, loss: 617.441714\n",
      "Epoch 136, loss: 826.583380\n",
      "Epoch 137, loss: 655.680675\n",
      "Epoch 138, loss: 721.744692\n",
      "Epoch 139, loss: 823.600784\n",
      "Epoch 140, loss: 726.722118\n",
      "Epoch 141, loss: 946.325798\n",
      "Epoch 142, loss: 792.882554\n",
      "Epoch 143, loss: 878.596083\n",
      "Epoch 144, loss: 715.291011\n",
      "Epoch 145, loss: 720.693322\n",
      "Epoch 146, loss: 676.779952\n",
      "Epoch 147, loss: 820.282302\n",
      "Epoch 148, loss: 685.850857\n",
      "Epoch 149, loss: 870.129487\n",
      "Epoch 150, loss: 839.522966\n",
      "Epoch 151, loss: 958.392959\n",
      "Epoch 152, loss: 641.692157\n",
      "Epoch 153, loss: 764.974544\n",
      "Epoch 154, loss: 636.015786\n",
      "Epoch 155, loss: 672.988993\n",
      "Epoch 156, loss: 802.759102\n",
      "Epoch 157, loss: 759.409179\n",
      "Epoch 158, loss: 741.359772\n",
      "Epoch 159, loss: 751.544718\n",
      "Epoch 160, loss: 764.545196\n",
      "Epoch 161, loss: 678.069985\n",
      "Epoch 162, loss: 668.990577\n",
      "Epoch 163, loss: 819.884573\n",
      "Epoch 164, loss: 668.612206\n",
      "Epoch 165, loss: 1022.893909\n",
      "Epoch 166, loss: 884.126641\n",
      "Epoch 167, loss: 618.180865\n",
      "Epoch 168, loss: 754.966090\n",
      "Epoch 169, loss: 816.885361\n",
      "Epoch 170, loss: 694.545242\n",
      "Epoch 171, loss: 648.417549\n",
      "Epoch 172, loss: 888.328849\n",
      "Epoch 173, loss: 731.827119\n",
      "Epoch 174, loss: 863.068799\n",
      "Epoch 175, loss: 766.185952\n",
      "Epoch 176, loss: 832.125515\n",
      "Epoch 177, loss: 765.287491\n",
      "Epoch 178, loss: 767.203719\n",
      "Epoch 179, loss: 881.825766\n",
      "Epoch 180, loss: 665.167618\n",
      "Epoch 181, loss: 739.987022\n",
      "Epoch 182, loss: 868.693882\n",
      "Epoch 183, loss: 770.309092\n",
      "Epoch 184, loss: 706.923271\n",
      "Epoch 185, loss: 662.853404\n",
      "Epoch 186, loss: 734.594496\n",
      "Epoch 187, loss: 695.210909\n",
      "Epoch 188, loss: 724.381984\n",
      "Epoch 189, loss: 887.095478\n",
      "Epoch 190, loss: 636.379381\n",
      "Epoch 191, loss: 686.030096\n",
      "Epoch 192, loss: 665.517414\n",
      "Epoch 193, loss: 635.618611\n",
      "Epoch 194, loss: 635.896859\n",
      "Epoch 195, loss: 825.024102\n",
      "Epoch 196, loss: 717.128715\n",
      "Epoch 197, loss: 835.486582\n",
      "Epoch 198, loss: 693.207103\n",
      "Epoch 199, loss: 701.892730\n",
      "Validation accuracy:  0.184 lr:  0.001 reg:  1e-05\n",
      "Epoch 0, loss: 888.679404\n",
      "Epoch 1, loss: 942.732064\n",
      "Epoch 2, loss: 907.380502\n",
      "Epoch 3, loss: 948.319788\n",
      "Epoch 4, loss: 837.518354\n",
      "Epoch 5, loss: 727.962711\n",
      "Epoch 6, loss: 708.678414\n",
      "Epoch 7, loss: 813.737315\n",
      "Epoch 8, loss: 871.006801\n",
      "Epoch 9, loss: 873.891754\n",
      "Epoch 10, loss: 949.731884\n",
      "Epoch 11, loss: 704.372657\n",
      "Epoch 12, loss: 745.706616\n",
      "Epoch 13, loss: 678.178815\n",
      "Epoch 14, loss: 803.972736\n",
      "Epoch 15, loss: 816.196385\n",
      "Epoch 16, loss: 827.285695\n",
      "Epoch 17, loss: 727.634801\n",
      "Epoch 18, loss: 797.575013\n",
      "Epoch 19, loss: 847.589858\n",
      "Epoch 20, loss: 874.382958\n",
      "Epoch 21, loss: 947.005247\n",
      "Epoch 22, loss: 1029.691910\n",
      "Epoch 23, loss: 763.386831\n",
      "Epoch 24, loss: 735.502012\n",
      "Epoch 25, loss: 859.206623\n",
      "Epoch 26, loss: 719.648976\n",
      "Epoch 27, loss: 704.527230\n",
      "Epoch 28, loss: 846.562305\n",
      "Epoch 29, loss: 728.310048\n",
      "Epoch 30, loss: 728.336606\n",
      "Epoch 31, loss: 796.612318\n",
      "Epoch 32, loss: 669.013538\n",
      "Epoch 33, loss: 781.875394\n",
      "Epoch 34, loss: 686.271230\n",
      "Epoch 35, loss: 832.432006\n",
      "Epoch 36, loss: 989.728031\n",
      "Epoch 37, loss: 710.390838\n",
      "Epoch 38, loss: 716.633898\n",
      "Epoch 39, loss: 772.401163\n",
      "Epoch 40, loss: 729.238985\n",
      "Epoch 41, loss: 962.075567\n",
      "Epoch 42, loss: 721.444038\n",
      "Epoch 43, loss: 845.016207\n",
      "Epoch 44, loss: 783.898233\n",
      "Epoch 45, loss: 804.255086\n",
      "Epoch 46, loss: 721.781615\n",
      "Epoch 47, loss: 765.160876\n",
      "Epoch 48, loss: 782.344322\n",
      "Epoch 49, loss: 819.785396\n",
      "Epoch 50, loss: 791.961409\n",
      "Epoch 51, loss: 880.879299\n",
      "Epoch 52, loss: 724.779157\n",
      "Epoch 53, loss: 742.326728\n",
      "Epoch 54, loss: 728.169861\n",
      "Epoch 55, loss: 879.889535\n",
      "Epoch 56, loss: 687.024105\n",
      "Epoch 57, loss: 903.725465\n",
      "Epoch 58, loss: 757.187984\n",
      "Epoch 59, loss: 990.269830\n",
      "Epoch 60, loss: 755.560178\n",
      "Epoch 61, loss: 957.296760\n",
      "Epoch 62, loss: 854.019958\n",
      "Epoch 63, loss: 782.974669\n",
      "Epoch 64, loss: 751.334007\n",
      "Epoch 65, loss: 777.165755\n",
      "Epoch 66, loss: 736.058942\n",
      "Epoch 67, loss: 819.531413\n",
      "Epoch 68, loss: 715.484378\n",
      "Epoch 69, loss: 790.271013\n",
      "Epoch 70, loss: 1084.600907\n",
      "Epoch 71, loss: 909.519344\n",
      "Epoch 72, loss: 735.535230\n",
      "Epoch 73, loss: 917.887246\n",
      "Epoch 74, loss: 814.979368\n",
      "Epoch 75, loss: 785.161482\n",
      "Epoch 76, loss: 627.289402\n",
      "Epoch 77, loss: 950.946125\n",
      "Epoch 78, loss: 657.132241\n",
      "Epoch 79, loss: 776.184221\n",
      "Epoch 80, loss: 795.539796\n",
      "Epoch 81, loss: 699.235085\n",
      "Epoch 82, loss: 794.538869\n",
      "Epoch 83, loss: 734.966166\n",
      "Epoch 84, loss: 832.406118\n",
      "Epoch 85, loss: 658.124140\n",
      "Epoch 86, loss: 692.581095\n",
      "Epoch 87, loss: 742.915787\n",
      "Epoch 88, loss: 692.883942\n",
      "Epoch 89, loss: 951.356314\n",
      "Epoch 90, loss: 863.345932\n",
      "Epoch 91, loss: 737.769779\n",
      "Epoch 92, loss: 922.235672\n",
      "Epoch 93, loss: 909.319693\n",
      "Epoch 94, loss: 1019.872570\n",
      "Epoch 95, loss: 649.267358\n",
      "Epoch 96, loss: 691.215740\n",
      "Epoch 97, loss: 777.404162\n",
      "Epoch 98, loss: 800.508876\n",
      "Epoch 99, loss: 733.122891\n",
      "Epoch 100, loss: 740.947044\n",
      "Epoch 101, loss: 818.102045\n",
      "Epoch 102, loss: 818.231494\n",
      "Epoch 103, loss: 784.515562\n",
      "Epoch 104, loss: 724.075539\n",
      "Epoch 105, loss: 813.382899\n",
      "Epoch 106, loss: 704.348335\n",
      "Epoch 107, loss: 798.452785\n",
      "Epoch 108, loss: 673.220310\n",
      "Epoch 109, loss: 795.257237\n",
      "Epoch 110, loss: 671.664535\n",
      "Epoch 111, loss: 767.566275\n",
      "Epoch 112, loss: 983.403929\n",
      "Epoch 113, loss: 757.388296\n",
      "Epoch 114, loss: 882.504088\n",
      "Epoch 115, loss: 715.051646\n",
      "Epoch 116, loss: 795.659936\n",
      "Epoch 117, loss: 780.825296\n",
      "Epoch 118, loss: 690.026002\n",
      "Epoch 119, loss: 707.496870\n",
      "Epoch 120, loss: 849.991280\n",
      "Epoch 121, loss: 689.554446\n",
      "Epoch 122, loss: 767.059804\n",
      "Epoch 123, loss: 800.745526\n",
      "Epoch 124, loss: 800.162833\n",
      "Epoch 125, loss: 791.720197\n",
      "Epoch 126, loss: 705.019764\n",
      "Epoch 127, loss: 747.048266\n",
      "Epoch 128, loss: 632.747831\n",
      "Epoch 129, loss: 756.289199\n",
      "Epoch 130, loss: 682.915934\n",
      "Epoch 131, loss: 744.293009\n",
      "Epoch 132, loss: 859.229610\n",
      "Epoch 133, loss: 733.099097\n",
      "Epoch 134, loss: 746.402687\n",
      "Epoch 135, loss: 694.519656\n",
      "Epoch 136, loss: 701.671802\n",
      "Epoch 137, loss: 859.962204\n",
      "Epoch 138, loss: 764.061003\n",
      "Epoch 139, loss: 928.380289\n",
      "Epoch 140, loss: 680.974888\n",
      "Epoch 141, loss: 766.594891\n",
      "Epoch 142, loss: 848.058344\n",
      "Epoch 143, loss: 661.425594\n",
      "Epoch 144, loss: 838.662973\n",
      "Epoch 145, loss: 682.785671\n",
      "Epoch 146, loss: 984.988073\n",
      "Epoch 147, loss: 771.686115\n",
      "Epoch 148, loss: 727.916094\n",
      "Epoch 149, loss: 752.321648\n",
      "Epoch 150, loss: 834.311978\n",
      "Epoch 151, loss: 886.691255\n",
      "Epoch 152, loss: 812.392507\n",
      "Epoch 153, loss: 776.083773\n",
      "Epoch 154, loss: 682.729523\n",
      "Epoch 155, loss: 754.315243\n",
      "Epoch 156, loss: 769.036481\n",
      "Epoch 157, loss: 821.851288\n",
      "Epoch 158, loss: 672.421459\n",
      "Epoch 159, loss: 612.633120\n",
      "Epoch 160, loss: 630.270962\n",
      "Epoch 161, loss: 685.017807\n",
      "Epoch 162, loss: 680.318927\n",
      "Epoch 163, loss: 653.299214\n",
      "Epoch 164, loss: 794.271547\n",
      "Epoch 165, loss: 794.533620\n",
      "Epoch 166, loss: 827.067643\n",
      "Epoch 167, loss: 756.227358\n",
      "Epoch 168, loss: 644.474567\n",
      "Epoch 169, loss: 618.760270\n",
      "Epoch 170, loss: 658.680205\n",
      "Epoch 171, loss: 761.217681\n",
      "Epoch 172, loss: 921.314418\n",
      "Epoch 173, loss: 650.621584\n",
      "Epoch 174, loss: 661.568627\n",
      "Epoch 175, loss: 824.786683\n",
      "Epoch 176, loss: 703.380525\n",
      "Epoch 177, loss: 933.304437\n",
      "Epoch 178, loss: 686.814479\n",
      "Epoch 179, loss: 809.754037\n",
      "Epoch 180, loss: 864.316124\n",
      "Epoch 181, loss: 746.617408\n",
      "Epoch 182, loss: 727.824024\n",
      "Epoch 183, loss: 708.221280\n",
      "Epoch 184, loss: 677.472797\n",
      "Epoch 185, loss: 795.724183\n",
      "Epoch 186, loss: 600.728047\n",
      "Epoch 187, loss: 936.208663\n",
      "Epoch 188, loss: 741.119103\n",
      "Epoch 189, loss: 835.147626\n",
      "Epoch 190, loss: 710.375744\n",
      "Epoch 191, loss: 753.004571\n",
      "Epoch 192, loss: 728.594414\n",
      "Epoch 193, loss: 726.217919\n",
      "Epoch 194, loss: 654.408911\n",
      "Epoch 195, loss: 668.699747\n",
      "Epoch 196, loss: 703.136797\n",
      "Epoch 197, loss: 682.767414\n",
      "Epoch 198, loss: 733.173915\n",
      "Epoch 199, loss: 729.914267\n",
      "Validation accuracy:  0.192 lr:  0.001 reg:  0.0001\n",
      "Epoch 0, loss: 880.578364\n",
      "Epoch 1, loss: 905.790077\n",
      "Epoch 2, loss: 952.982563\n",
      "Epoch 3, loss: 857.470614\n",
      "Epoch 4, loss: 911.689242\n",
      "Epoch 5, loss: 777.425762\n",
      "Epoch 6, loss: 848.193814\n",
      "Epoch 7, loss: 682.791215\n",
      "Epoch 8, loss: 1044.364784\n",
      "Epoch 9, loss: 881.647756\n",
      "Epoch 10, loss: 786.641840\n",
      "Epoch 11, loss: 1012.052659\n",
      "Epoch 12, loss: 807.941087\n",
      "Epoch 13, loss: 848.859520\n",
      "Epoch 14, loss: 791.128356\n",
      "Epoch 15, loss: 819.534034\n",
      "Epoch 16, loss: 991.186403\n",
      "Epoch 17, loss: 709.871805\n",
      "Epoch 18, loss: 892.553733\n",
      "Epoch 19, loss: 808.709236\n",
      "Epoch 20, loss: 834.417713\n",
      "Epoch 21, loss: 828.944824\n",
      "Epoch 22, loss: 773.861751\n",
      "Epoch 23, loss: 805.795581\n",
      "Epoch 24, loss: 829.520127\n",
      "Epoch 25, loss: 962.215339\n",
      "Epoch 26, loss: 732.644510\n",
      "Epoch 27, loss: 716.952391\n",
      "Epoch 28, loss: 781.871717\n",
      "Epoch 29, loss: 769.609200\n",
      "Epoch 30, loss: 801.516463\n",
      "Epoch 31, loss: 812.089634\n",
      "Epoch 32, loss: 918.479193\n",
      "Epoch 33, loss: 738.596629\n",
      "Epoch 34, loss: 737.306761\n",
      "Epoch 35, loss: 755.304407\n",
      "Epoch 36, loss: 685.976679\n",
      "Epoch 37, loss: 847.220664\n",
      "Epoch 38, loss: 805.676159\n",
      "Epoch 39, loss: 719.275023\n",
      "Epoch 40, loss: 859.787965\n",
      "Epoch 41, loss: 711.190705\n",
      "Epoch 42, loss: 766.649262\n",
      "Epoch 43, loss: 825.134381\n",
      "Epoch 44, loss: 789.166582\n",
      "Epoch 45, loss: 702.818868\n",
      "Epoch 46, loss: 649.970560\n",
      "Epoch 47, loss: 654.917672\n",
      "Epoch 48, loss: 890.739141\n",
      "Epoch 49, loss: 742.297799\n",
      "Epoch 50, loss: 720.634646\n",
      "Epoch 51, loss: 677.425106\n",
      "Epoch 52, loss: 765.926263\n",
      "Epoch 53, loss: 749.289074\n",
      "Epoch 54, loss: 708.517806\n",
      "Epoch 55, loss: 733.511690\n",
      "Epoch 56, loss: 637.435142\n",
      "Epoch 57, loss: 771.950778\n",
      "Epoch 58, loss: 680.000740\n",
      "Epoch 59, loss: 804.281975\n",
      "Epoch 60, loss: 806.290198\n",
      "Epoch 61, loss: 660.975102\n",
      "Epoch 62, loss: 689.180521\n",
      "Epoch 63, loss: 731.204680\n",
      "Epoch 64, loss: 760.135392\n",
      "Epoch 65, loss: 704.959643\n",
      "Epoch 66, loss: 891.762182\n",
      "Epoch 67, loss: 783.740091\n",
      "Epoch 68, loss: 608.907151\n",
      "Epoch 69, loss: 774.380462\n",
      "Epoch 70, loss: 746.063673\n",
      "Epoch 71, loss: 718.994412\n",
      "Epoch 72, loss: 994.512684\n",
      "Epoch 73, loss: 895.023886\n",
      "Epoch 74, loss: 801.755718\n",
      "Epoch 75, loss: 643.938897\n",
      "Epoch 76, loss: 688.775430\n",
      "Epoch 77, loss: 773.459028\n",
      "Epoch 78, loss: 675.798394\n",
      "Epoch 79, loss: 743.558702\n",
      "Epoch 80, loss: 739.325570\n",
      "Epoch 81, loss: 731.796489\n",
      "Epoch 82, loss: 834.697054\n",
      "Epoch 83, loss: 924.286172\n",
      "Epoch 84, loss: 829.646262\n",
      "Epoch 85, loss: 762.496533\n",
      "Epoch 86, loss: 865.537415\n",
      "Epoch 87, loss: 758.338863\n",
      "Epoch 88, loss: 688.353731\n",
      "Epoch 89, loss: 799.850256\n",
      "Epoch 90, loss: 688.895586\n",
      "Epoch 91, loss: 891.582884\n",
      "Epoch 92, loss: 742.381482\n",
      "Epoch 93, loss: 875.944343\n",
      "Epoch 94, loss: 740.950983\n",
      "Epoch 95, loss: 752.849272\n",
      "Epoch 96, loss: 821.086697\n",
      "Epoch 97, loss: 779.445007\n",
      "Epoch 98, loss: 800.056991\n",
      "Epoch 99, loss: 872.020211\n",
      "Epoch 100, loss: 672.303792\n",
      "Epoch 101, loss: 855.792001\n",
      "Epoch 102, loss: 694.968375\n",
      "Epoch 103, loss: 780.888841\n",
      "Epoch 104, loss: 793.211397\n",
      "Epoch 105, loss: 846.926034\n",
      "Epoch 106, loss: 803.079674\n",
      "Epoch 107, loss: 691.859127\n",
      "Epoch 108, loss: 667.586625\n",
      "Epoch 109, loss: 649.165434\n",
      "Epoch 110, loss: 759.242705\n",
      "Epoch 111, loss: 811.496695\n",
      "Epoch 112, loss: 697.197851\n",
      "Epoch 113, loss: 759.786139\n",
      "Epoch 114, loss: 872.381745\n",
      "Epoch 115, loss: 664.927343\n",
      "Epoch 116, loss: 804.270849\n",
      "Epoch 117, loss: 761.679942\n",
      "Epoch 118, loss: 729.816984\n",
      "Epoch 119, loss: 773.232877\n",
      "Epoch 120, loss: 713.172794\n",
      "Epoch 121, loss: 839.184867\n",
      "Epoch 122, loss: 788.754073\n",
      "Epoch 123, loss: 869.937988\n",
      "Epoch 124, loss: 743.281995\n",
      "Epoch 125, loss: 770.198702\n",
      "Epoch 126, loss: 708.808198\n",
      "Epoch 127, loss: 707.721265\n",
      "Epoch 128, loss: 690.088951\n",
      "Epoch 129, loss: 659.903408\n",
      "Epoch 130, loss: 766.306704\n",
      "Epoch 131, loss: 767.384331\n",
      "Epoch 132, loss: 814.792143\n",
      "Epoch 133, loss: 888.990319\n",
      "Epoch 134, loss: 702.119502\n",
      "Epoch 135, loss: 785.614153\n",
      "Epoch 136, loss: 678.788657\n",
      "Epoch 137, loss: 650.006277\n",
      "Epoch 138, loss: 708.967260\n",
      "Epoch 139, loss: 775.093902\n",
      "Epoch 140, loss: 760.963421\n",
      "Epoch 141, loss: 629.251924\n",
      "Epoch 142, loss: 758.003524\n",
      "Epoch 143, loss: 834.225803\n",
      "Epoch 144, loss: 741.767309\n",
      "Epoch 145, loss: 761.720544\n",
      "Epoch 146, loss: 871.188713\n",
      "Epoch 147, loss: 713.529191\n",
      "Epoch 148, loss: 873.250296\n",
      "Epoch 149, loss: 687.511251\n",
      "Epoch 150, loss: 678.585991\n",
      "Epoch 151, loss: 751.593963\n",
      "Epoch 152, loss: 788.549772\n",
      "Epoch 153, loss: 801.567465\n",
      "Epoch 154, loss: 812.931598\n",
      "Epoch 155, loss: 721.270912\n",
      "Epoch 156, loss: 910.022414\n",
      "Epoch 157, loss: 723.345116\n",
      "Epoch 158, loss: 736.166214\n",
      "Epoch 159, loss: 840.021302\n",
      "Epoch 160, loss: 752.302795\n",
      "Epoch 161, loss: 786.541365\n",
      "Epoch 162, loss: 645.731639\n",
      "Epoch 163, loss: 776.754395\n",
      "Epoch 164, loss: 863.704169\n",
      "Epoch 165, loss: 881.346736\n",
      "Epoch 166, loss: 641.818518\n",
      "Epoch 167, loss: 666.959577\n",
      "Epoch 168, loss: 776.102224\n",
      "Epoch 169, loss: 641.567029\n",
      "Epoch 170, loss: 724.747621\n",
      "Epoch 171, loss: 668.596020\n",
      "Epoch 172, loss: 950.755011\n",
      "Epoch 173, loss: 677.057981\n",
      "Epoch 174, loss: 731.491359\n",
      "Epoch 175, loss: 686.888876\n",
      "Epoch 176, loss: 903.245970\n",
      "Epoch 177, loss: 695.574764\n",
      "Epoch 178, loss: 794.459858\n",
      "Epoch 179, loss: 802.053203\n",
      "Epoch 180, loss: 792.746013\n",
      "Epoch 181, loss: 768.830782\n",
      "Epoch 182, loss: 768.340884\n",
      "Epoch 183, loss: 722.597219\n",
      "Epoch 184, loss: 708.428375\n",
      "Epoch 185, loss: 640.581140\n",
      "Epoch 186, loss: 811.319554\n",
      "Epoch 187, loss: 853.967065\n",
      "Epoch 188, loss: 664.955814\n",
      "Epoch 189, loss: 697.699467\n",
      "Epoch 190, loss: 633.003984\n",
      "Epoch 191, loss: 704.567016\n",
      "Epoch 192, loss: 777.158026\n",
      "Epoch 193, loss: 742.278964\n",
      "Epoch 194, loss: 711.777117\n",
      "Epoch 195, loss: 620.162202\n",
      "Epoch 196, loss: 769.870558\n",
      "Epoch 197, loss: 735.604673\n",
      "Epoch 198, loss: 812.125107\n",
      "Epoch 199, loss: 713.235804\n",
      "Validation accuracy:  0.194 lr:  0.001 reg:  0.001\n",
      "best validation accuracy achieved: 0.251000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "reg_strengths = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print(\"Validation accuracy: \", accuracy, \"lr: \", lr, \"reg: \", reg)\n",
    "        if best_val_accuracy is None or accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.214000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
